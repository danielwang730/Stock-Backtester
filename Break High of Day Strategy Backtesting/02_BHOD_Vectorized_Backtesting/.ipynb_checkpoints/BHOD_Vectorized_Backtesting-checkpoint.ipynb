{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "116c2d1d-9612-4925-b901-60ced0845635",
   "metadata": {},
   "source": [
    "# Break High of Day (BHOD) Strategy Vectorized Backtesting\n",
    "In this notebook, instead of performing event-driven backtests, we're vectorizing the process, which exponentially decreases the amount of time it takes for the backtests to run.\n",
    "\n",
    "Once again, we will specifically focus on the Break High of Day (BHOD) strategy, as it's relatively simple to understand and implement, and we believe it has a high success rate. Furthermore, we believe that we can use this backtesting method to fine-tune the BHOD strategy even further, as we can alter different parameters of the strategy and see how it would have performed with historical data. Once we find the optimal parameters, we ultimately hope to test this strategy in the stock market to see how well it can do."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c82f3c1-c5b4-4aa8-83e4-5848140f9b12",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Importing Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d26ba70e-de1b-4df1-8713-935bc6282627",
   "metadata": {},
   "outputs": [],
   "source": [
    "from backtesting import Backtest, Strategy\n",
    "from backtesting.backtesting import Order, Position, Trade, _Broker\n",
    "from backtesting.test import GOOG\n",
    "from backtesting.lib import crossover\n",
    "\n",
    "import tulipy as ti\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "from datetime import datetime\n",
    "import time\n",
    "import dask\n",
    "import dask.dataframe as dd\n",
    "from dask import delayed\n",
    "\n",
    "import os\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7730909b-ae4c-4fa6-befe-0e3dcc7187af",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Data Transformation / Cleaning Functions\n",
    "These functions will be used to clean our data for us to use later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "824d8470-6bde-445e-a750-c81778609d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for converting timestamp and setting it as index\n",
    "def get_true_timestamp_and_set_as_index(pd_csv_ticker_1min):\n",
    "    # Converting timestamp to actual timestamp\n",
    "    pd_csv_ticker_1min['timestamp'] = pd.to_datetime(pd_csv_ticker_1min['timestamp'])\n",
    "    \n",
    "    # Setting the index to the timestamp\n",
    "    pd_csv_ticker_1min.set_index('timestamp', inplace=True)\n",
    "\n",
    "    return pd_csv_ticker_1min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4d5c0e25-d3a4-4646-97ca-e72a637fccca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for filling in missing minutes\n",
    "def fill_missing_minutes(index_timestamp_1_min, keep_only_PM_to_TD = False, PM_cutoff_time ='04:00'):\n",
    "    index_timestamp_1_min = index_timestamp_1_min.resample('min').asfreq()\n",
    "\n",
    "    # Filling in NaN values (volume with 0 and close to the last known close)\n",
    "    index_timestamp_1_min['volume'] = index_timestamp_1_min['volume'].fillna(0);\n",
    "    index_timestamp_1_min['close'] = index_timestamp_1_min['close'].ffill();\n",
    "    \n",
    "    # Filling in the rest, with the NaN being equal to the previous close\n",
    "    index_timestamp_1_min['open'] = index_timestamp_1_min['open'].fillna(index_timestamp_1_min['close']);\n",
    "    index_timestamp_1_min['high'] = index_timestamp_1_min['high'].fillna(index_timestamp_1_min['close']);\n",
    "    index_timestamp_1_min['low'] = index_timestamp_1_min['low'].fillna(index_timestamp_1_min['close']);\n",
    "    \n",
    "    # Making sure there are only weekdays\n",
    "    index_timestamp_1_min = index_timestamp_1_min[index_timestamp_1_min.index.weekday < 5]\n",
    "\n",
    "    # Keeping only data from PM_cutoff_time to 3:59pm (if keep_only_PM_to_TD is True)\n",
    "    if keep_only_PM_to_TD:\n",
    "        index_timestamp_1_min = index_timestamp_1_min.between_time(PM_cutoff_time, '15:59')\n",
    "\n",
    "    return index_timestamp_1_min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e6292a5a-cf68-45f8-92e6-305f24ebed35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renaming columns with uppercase\n",
    "def upper_case_OHLCV(no_missing_1_min):\n",
    "    no_missing_1_min.rename(columns = {'open':'Open', 'high':'High', 'low':'Low', 'close':'Close', 'volume':'Volume'}, inplace = True)\n",
    "\n",
    "    return no_missing_1_min"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "550a7db1-ac50-4f41-a461-ef2bfb04b392",
   "metadata": {},
   "source": [
    "# Vectorized Backtesting and Metrics Analysis\n",
    "For the vectorized backtests, we'll use the dask dataframes like we did previously with the event-driven backtests. However, we'll combine that with new features that can only be done in a vectorized manner, and we'll also be implementing slightly different partitions to speed up the tests."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61712f58-6e44-4da4-af5e-70a7fce88c08",
   "metadata": {},
   "source": [
    "## Creating a Dictionary of Variables to Input\n",
    "We create a dictionary of variables (variables_dict) that we can plug into our vectorized backtest to change our outcomes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a40516f4-01d5-45a3-b561-6c62f7fbaf94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the dictionary of variables\n",
    "variables_dict = {\n",
    "    'Trading Start Time (HH:MM)': '09:35',\n",
    "    'Trading End Time (HH:MM)': '15:55',\n",
    "    'Use Volume Condition': True,\n",
    "    'Volume Percent List (%)': [10],\n",
    "    'Set Volume Average Period (#)': 14,\n",
    "    'Risk per Trade R ($)': 20,\n",
    "    'Bid-Ask Spread ($)': 0.06,\n",
    "    'Commission per Share ($)': 0.0035,\n",
    "    'Min Comm per Order ($)': 0.35,\n",
    "    'Exchange Fees ($)': 0.003,\n",
    "    'Other Fees ($)': 0.0005,\n",
    "    'Max Trade Amount ($)': 50_000,\n",
    "    'Use 1st R tp': True,\n",
    "    'Set 1st R partial (#)': 1,\n",
    "    'Set 1st R percent (%)': 25,\n",
    "    'Use 2nd R tp': True,\n",
    "    'Set 2nd R partial (#)': 2,\n",
    "    'Set 2nd R percent (%)': 25,\n",
    "    'Use 3rd R tp': True,\n",
    "    'Set 3rd R partial (#)': 3,\n",
    "    'Set 3rd R percent (%)': 50,\n",
    "    'Use 4th R tp': False,\n",
    "    'Set 4th R partial (#)': 4,\n",
    "    'Set 4th R percent (%)': 0,\n",
    "    'Move SL After 1st tp': True,\n",
    "    'Move SL After 2nd tp': False,\n",
    "    'Move SL After 3rd tp': False\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1bd4c3bf-31f5-49f3-8532-f211142942ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting an original dictionary of variables (copied from above, but just easy for resetting purposes)\n",
    "variables_dict_orig = {\n",
    "    'Trading Start Time (HH:MM)': '09:35',\n",
    "    'Trading End Time (HH:MM)': '15:55',\n",
    "    'Use Volume Condition': True,\n",
    "    'Volume Percent List (%)': [10],\n",
    "    'Set Volume Average Period (#)': 14,\n",
    "    'Risk per Trade R ($)': 20,\n",
    "    'Bid-Ask Spread ($)': 0.06,\n",
    "    'Commission per Share ($)': 0.0035,\n",
    "    'Min Comm per Order ($)': 0.35,\n",
    "    'Exchange Fees ($)': 0.003,\n",
    "    'Other Fees ($)': 0.0005,\n",
    "    'Max Trade Amount ($)': 50_000,\n",
    "    'Use 1st R tp': True,\n",
    "    'Set 1st R partial (#)': 1,\n",
    "    'Set 1st R percent (%)': 25,\n",
    "    'Use 2nd R tp': True,\n",
    "    'Set 2nd R partial (#)': 2,\n",
    "    'Set 2nd R percent (%)': 25,\n",
    "    'Use 3rd R tp': True,\n",
    "    'Set 3rd R partial (#)': 3,\n",
    "    'Set 3rd R percent (%)': 50,\n",
    "    'Use 4th R tp': False,\n",
    "    'Set 4th R partial (#)': 4,\n",
    "    'Set 4th R percent (%)': 0,\n",
    "    'Move SL After 1st tp': True,\n",
    "    'Move SL After 2nd tp': False,\n",
    "    'Move SL After 3rd tp': False\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a97c21dd-307f-4d1f-816e-49d2ef1f7d71",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Delayed Function for Vectorized Backtesting\n",
    "We create this delayed function so we can compute (i.e., execute) the backtest when we're ready."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ce68378b-fa0a-46ad-bc1b-3799a736db00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a delayed function that basically doesn't compute until dask.compute() is run (which allows the parallel running to execute)\n",
    "@delayed\n",
    "def run_vec_backtest_on_part(data_partition, backtest_strategy, var_dict):\n",
    "    \n",
    "    if data_partition.empty:  # Skip if the partition is on the weekend or holiday (and thus will have empty data)\n",
    "        return None\n",
    "\n",
    "    trade_log = backtest_strategy(data_partition, var_dict)\n",
    "\n",
    "    return trade_log"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0836c83a-0012-485a-9b56-ac6605628557",
   "metadata": {},
   "source": [
    "## Vectorized Backtesting Function\n",
    "Instead of having a singular class, we simply have a function that can run our backtests for us. In this vectorized backtesting format, instead of sequentially running through all the data, we can utilize the pandas to vectorize that process using a different set of logic. This allows for much faster computation and data processing, and it exponentially reduces the amount of time we need to run these backtests.\n",
    "\n",
    "If you're unfamiliar with the Break High of Day strategy itself, there are YouTube videos and other websites (e.g., Bear Bull Traders) that explain the strategy extensively. For a simplified summary, the strategy involves checking when the price of a stock breaks its previous high of the day, and after confirming a series of conditions, enters a long trade. To exit a trade, one of three conditions must be met: 1) at least the high of a 1-minute candlestick (CS) reaches a desired take-profit (TP) level, 2) at least the low of a 1-minute CS reaches the stop loss (SL) or breakeven point (BE), or 3) we reach 3:55pm with neither previous condition being met. In all cases above, we only trade during the regular trading hours, and we never hold overnight positions.\n",
    "\n",
    "In addition to the concepts mentioned above, this vectorized backtest allows us to test different volume conditions with minimal time increase, and it fully allows for variable take-profit levels and variable SL and BE positions. We can see some of the results from running this backtest below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4de7c927-6460-4c4f-89b0-68cb1331d7e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BHOD Full Vectorized Strategy\n",
    "def BHOD_Full_Vec_v1(partitioned_data, var_dict):\n",
    "    \n",
    "    # Making a copy of the partitioned data\n",
    "    df_1min = partitioned_data.copy()\n",
    "\n",
    "    # Part 1: Initializing variables and indicators to get the final BHOD condition:\n",
    "\n",
    "    # Getting date and time separate for easier use\n",
    "    df_1min['date'] = df_1min.index.date\n",
    "    df_1min['time'] = df_1min.index.time\n",
    "\n",
    "    # Making sure we're only trading within proper trading hours\n",
    "    start_time = var_dict['Trading Start Time (HH:MM)']\n",
    "    end_time = var_dict['Trading End Time (HH:MM)']\n",
    "\n",
    "    df_1min['valid_trading_time'] = (\n",
    "        df_1min.time >= pd.to_datetime(start_time).time()) & (df_1min.time <= pd.to_datetime(end_time).time())\n",
    "\n",
    "    # Defining a function to add the moving averages\n",
    "    def add_ma(df, period, ma_col, new_col_name):\n",
    "        df[new_col_name] = df[ma_col].rolling(period).mean()\n",
    "\n",
    "    # Adding volume moving average and another column with volume thresholds of XX%\n",
    "    add_ma(df_1min, var_dict['Set Volume Average Period (#)'], 'Volume', 'volume_MA')\n",
    "    volume_percent_list = var_dict['Volume Percent List (%)']\n",
    "    volume_percent_list.sort()  # Sorting lowest to highest\n",
    "\n",
    "    # Creating counter for 1st volume condition, indic cols for final trades table, dict of volume index lists, and recent exits dict\n",
    "    volume_cond_count = 0  # Counter for first volume condition\n",
    "    vol_conds_list = []  # Indicator columns for final trades table\n",
    "    vol_cond_dict = {}  # Initializing a dictionary of lists for true volume condition indexes at each threshold\n",
    "    recent_exits_dict = {}  # Recent exits dictionary\n",
    "    \n",
    "    for vol_perc in volume_percent_list:\n",
    "        vol_thresh_name = 'w_vol_thresh_' + str(vol_perc) + '_perc'  # Setting volume threshold name\n",
    "        vol_thresh_name_series = df_1min['volume_MA'] * (1 + vol_perc / 100)  # Setting volume threshold\n",
    "\n",
    "        # Adding in volume indicator (if \"Use Volume Condition\" is True)\n",
    "        vol_cond_name = 'volume_condition_' + str(vol_perc)\n",
    "        vol_cond_name_series = not var_dict['Use Volume Condition'] or (df_1min['Volume'] >= vol_thresh_name_series)\n",
    "\n",
    "        # Setting the first (lowest) volume percent as the first condition\n",
    "        if not volume_cond_count:  # If it's not False (i.e., if volume_cond_count is 0, execute this statement)\n",
    "            df_1min['volume_condition_first'] = vol_cond_name_series\n",
    "        volume_cond_count += 1\n",
    "\n",
    "        # Appending volume condition name to vol_conds_list\n",
    "        vol_conds_list.append(vol_cond_name)\n",
    "\n",
    "        # Inputting the lists of indexes that meet the volume condition threshold into vol_cond_dict\n",
    "        vol_cond_dict[vol_cond_name] = vol_cond_name_series[vol_cond_name_series].index.tolist()\n",
    "        \n",
    "        # Initializing the most recent exits for each condition (starting with some early date)\n",
    "        recent_exits_dict[vol_cond_name] = [pd.to_datetime('1950-01-01 09:00')]\n",
    "    \n",
    "    # Getting the HOD and previous HOD\n",
    "    _filtered1 = df_1min[df_1min['time'] >= pd.to_datetime('09:30:00').time()].copy()  # Filtering rows starting at 9:30am each day\n",
    "    _filtered1['HOD'] = _filtered1.groupby('date')['High'].cummax()  # Computing HOD starting at 9:30am each day\n",
    "    df_1min = df_1min.merge(\n",
    "        _filtered1[['HOD']], how = 'left', left_index = True, right_index = True)  # Merging HOD column back to original df\n",
    "    df_1min['prev_HOD'] = df_1min.HOD.shift(1)  # Getting the prev_HOD\n",
    "\n",
    "    # Setting condition that the current high must be greater than previous HOD\n",
    "    df_1min['broke_prev_HOD_cond'] = df_1min.High > df_1min.prev_HOD\n",
    "\n",
    "    # Setting non-continuous BHOD condition (i.e., previous 2 CS did not also break HOD)\n",
    "    df_1min['non_continuous_BHOD'] = (~df_1min.broke_prev_HOD_cond.shift(1).fillna(False).astype(bool)) & (\n",
    "        ~df_1min.broke_prev_HOD_cond.shift(2).fillna(False).astype(bool))\n",
    "\n",
    "    # Getting final BHOD_condition\n",
    "    df_1min['combined_BHOD_condition'] = (\n",
    "        df_1min.valid_trading_time & df_1min.volume_condition_first & df_1min.broke_prev_HOD_cond & df_1min.non_continuous_BHOD)\n",
    "\n",
    "    # Dropping unnecessary columns to save space\n",
    "    df_1min = df_1min.drop(['date', 'time', 'valid_trading_time', 'volume_MA', 'volume_condition_first', 'HOD', 'broke_prev_HOD_cond',\n",
    "        'non_continuous_BHOD'], axis=1)\n",
    "\n",
    "\n",
    "    # Part 2: Initializing entry and exit variables to ultimately perform a trade:\n",
    "\n",
    "    # Setting risk per trade in dollars (R)\n",
    "    risk_per_trade_R = var_dict['Risk per Trade R ($)']\n",
    "    \n",
    "    # Introducing slippage (from bid-ask spread) in\n",
    "    BA_spread = var_dict['Bid-Ask Spread ($)']\n",
    "    slippage = BA_spread / 2\n",
    "    \n",
    "    # Max trade amount:\n",
    "    max_trade_amount = var_dict['Max Trade Amount ($)']\n",
    "    \n",
    "    # Some useful metrics that will be used to check whether trade is above max trade amount\n",
    "    _sep_w_slippage = 0.0\n",
    "    _BHOD_SL = 0.0\n",
    "    _dynamic_shares_total = 0.0\n",
    "    _total_entry_amt = 0.0\n",
    "\n",
    "    # Setting partials and percent take profit (using R:R-based TP)\n",
    "    use_1st_RR_tp = var_dict['Use 1st R tp']\n",
    "    set_1st_R_partial = var_dict['Set 1st R partial (#)']\n",
    "    set_1st_R_percent = var_dict['Set 1st R percent (%)']\n",
    "    use_2nd_RR_tp = var_dict['Use 2nd R tp']\n",
    "    set_2nd_R_partial = var_dict['Set 2nd R partial (#)']\n",
    "    set_2nd_R_percent = var_dict['Set 2nd R percent (%)']\n",
    "    use_3rd_RR_tp = var_dict['Use 3rd R tp']\n",
    "    set_3rd_R_partial = var_dict['Set 3rd R partial (#)']\n",
    "    set_3rd_R_percent = var_dict['Set 3rd R percent (%)']\n",
    "    use_4th_RR_tp = var_dict['Use 4th R tp']\n",
    "    set_4th_R_partial = var_dict['Set 4th R partial (#)']\n",
    "    set_4th_R_percent = var_dict['Set 4th R percent (%)']\n",
    "    \n",
    "    # Initializing dynamic shares calculation\n",
    "    dynamic_shares_total = 0\n",
    "    dynamic_1st_R_shares = 0\n",
    "    dynamic_2nd_R_shares = 0\n",
    "    dynamic_3rd_R_shares = 0\n",
    "    dynamic_4th_R_shares = 0\n",
    "\n",
    "    # Initializing when to move stop loss\n",
    "    move_SL_after_1st_tp = var_dict['Move SL After 1st tp']\n",
    "    move_SL_after_2nd_tp = var_dict['Move SL After 2nd tp']\n",
    "    move_SL_after_3rd_tp = var_dict['Move SL After 3rd tp']\n",
    "\n",
    "\n",
    "    # Part 3: Initializing columns to run the backtest:\n",
    "\n",
    "    # Setting full entry condition that includes test for max entry amount\n",
    "    df_1min['full_entry_condition'] = False\n",
    "    \n",
    "    # Initializing stored entry price (sep) with slippage with NaN\n",
    "    df_1min['sep_w_slippage'] = np.nan\n",
    "    \n",
    "    # Defining the previous midpoint and individual stop losses\n",
    "    # (For BHOD, sl is at the lower of the current CS low or at the midpoint of the open and close of the previous CS)\n",
    "    prev_midpoint_series = (df_1min.Open.shift(1) + df_1min.Close.shift(1)) / 2\n",
    "    df_1min['indiv_sl'] = np.minimum(prev_midpoint_series, df_1min.Low)\n",
    "    \n",
    "    # Initializing true BHOD stop losses and breakeven price\n",
    "    df_1min['BHOD_SL'] = np.nan\n",
    "    df_1min['BHOD_BE'] = np.nan\n",
    "    \n",
    "    # Initializing the take profit prices\n",
    "    df_1min['TP_Long_1st_R_price'] = np.nan\n",
    "    df_1min['TP_Long_2nd_R_price'] = np.nan\n",
    "    df_1min['TP_Long_3rd_R_price'] = np.nan\n",
    "    df_1min['TP_Long_4th_R_price'] = np.nan\n",
    "    \n",
    "    # Setting selling conditions\n",
    "    df_1min['hit_SL'] = False\n",
    "    df_1min['hit_BE'] = False\n",
    "    df_1min['hit_1st_TP'] = False\n",
    "    df_1min['hit_2nd_TP'] = False\n",
    "    df_1min['hit_3rd_TP'] = False\n",
    "    df_1min['hit_4th_TP'] = False\n",
    "    \n",
    "    # Initializing take profit indicators  \n",
    "    df_1min['hit_SL_cum'] = 0.0\n",
    "    df_1min['hit_BE_cum'] = 0.0\n",
    "    df_1min['hit_1st_TP_cum'] = 0.0\n",
    "    df_1min['hit_2nd_TP_cum'] = 0.0\n",
    "    df_1min['hit_3rd_TP_cum'] = 0.0\n",
    "    df_1min['hit_4th_TP_cum'] = 0.0\n",
    "    \n",
    "    # Initializing the shares with NaN and exit amounts with 0\n",
    "    df_1min['shares'] = np.nan\n",
    "    df_1min['exit_amt_sl'] = 0.0\n",
    "    df_1min['exit_amt_be'] = 0.0\n",
    "    df_1min['exit_amt_1st_tp'] = 0.0\n",
    "    df_1min['exit_amt_2nd_tp'] = 0.0\n",
    "    df_1min['exit_amt_3rd_tp'] = 0.0\n",
    "    df_1min['exit_amt_4th_tp'] = 0.0\n",
    "    df_1min['exit_amt_3_55_pm'] = 0.0\n",
    "\n",
    "    # Initializing the trades table for analysis at the end\n",
    "    trades_table_cols = ['Size', 'EntryPrice', 'ExitPrice', 'PnL', 'PnL_AF', 'EntryTime', 'ExitTime', 'Duration'] + vol_conds_list\n",
    "    trades_table = pd.DataFrame(columns = trades_table_cols)\n",
    "\n",
    "\n",
    "    # Part 4: Getting potential entry points and executing the trades    \n",
    "    \n",
    "    # Getting the list of all potential entry points:\n",
    "    potential_entry_timestamps = df_1min[df_1min.combined_BHOD_condition].index.tolist()\n",
    "\n",
    "    # Executing the strategy for each timestamp\n",
    "    for timestamp in potential_entry_timestamps:\n",
    "    \n",
    "        # Step 1: Initializing some variables to make things easier\n",
    "        \n",
    "        # Getting a copy of just the timestamp row (observation) to make operations faster\n",
    "        timestamp_row = df_1min.loc[timestamp].copy(deep = True)\n",
    "\n",
    "        \n",
    "        # Step 2: Setting the entry price, stop loss, and take profit prices for this entry row (Series)\n",
    "    \n",
    "        # Setting variables for max entry amount condition (to be used later if this condition is met)\n",
    "        _sep_w_slippage = np.minimum((timestamp_row['prev_HOD'] + slippage), timestamp_row['High'])\n",
    "        # ^Setting stored entry price with slippage\n",
    "        _BHOD_SL = timestamp_row.indiv_sl  # Setting the true BHOD sl for this timestamp\n",
    "        _dynamic_shares_total = int(np.floor(risk_per_trade_R / (_sep_w_slippage - _BHOD_SL)))  # Setting the total dynamic shares\n",
    "        _total_entry_amt = _dynamic_shares_total * _sep_w_slippage  # Setting total purchase amount\n",
    "    \n",
    "        # Before doing anything else, checking to see if entry will be larger than the specified max entry amount\n",
    "        if _total_entry_amt > max_trade_amount:\n",
    "            continue\n",
    "        timestamp_row['full_entry_condition'] = True  # Setting full entry condition to True if max entry amount condition is met\n",
    "        \n",
    "        # Filling in the stored entry price with slippage, BHOD SL, and BHOD BE for this timestamp\n",
    "        timestamp_row['sep_w_slippage'] = _sep_w_slippage\n",
    "        timestamp_row['BHOD_SL'] = _BHOD_SL\n",
    "        timestamp_row['BHOD_BE'] = _sep_w_slippage\n",
    "    \n",
    "        # Setting the take profit prices for this timestamp\n",
    "        entry_minus_SL = _sep_w_slippage - _BHOD_SL\n",
    "        timestamp_row['TP_Long_1st_R_price'] = entry_minus_SL * set_1st_R_partial + _sep_w_slippage\n",
    "        timestamp_row['TP_Long_2nd_R_price'] = entry_minus_SL * set_2nd_R_partial + _sep_w_slippage\n",
    "        timestamp_row['TP_Long_3rd_R_price'] = entry_minus_SL * set_3rd_R_partial + _sep_w_slippage\n",
    "        timestamp_row['TP_Long_4th_R_price'] = entry_minus_SL * set_4th_R_partial + _sep_w_slippage\n",
    "\n",
    "        \n",
    "        # Step 3: Setting share sizes for everything\n",
    "\n",
    "        # Setting true dynamic shares total variable as the test variable for dynamic shares total\n",
    "        dynamic_shares_total = _dynamic_shares_total\n",
    "        \n",
    "        # Setting take profit share sizes (Note: depending on the % splits, the shares may sometimes be 0; but that's fine; let it be)\n",
    "        if use_1st_RR_tp:\n",
    "            dynamic_1st_R_shares = int(np.floor(dynamic_shares_total * set_1st_R_percent / 100))\n",
    "        # 2R is basically taking the remaining shares after 1st R and calculating 2nd R share size from that\n",
    "        # (instead of doing it all at once in the beginning)\n",
    "        if use_2nd_RR_tp:\n",
    "            dynamic_2nd_R_shares = int(np.floor(\n",
    "                (dynamic_shares_total - dynamic_1st_R_shares) * set_2nd_R_percent / (100 - set_1st_R_percent))  )\n",
    "        # 3R follows suit\n",
    "        if use_3rd_RR_tp:\n",
    "            dynamic_3rd_R_shares = int(np.floor(\n",
    "                (dynamic_shares_total - dynamic_1st_R_shares - dynamic_2nd_R_shares) * \n",
    "                set_3rd_R_percent / (100 - set_1st_R_percent - set_2nd_R_percent)) )\n",
    "        # 4R follows suit as well (if applied)\n",
    "        if use_4th_RR_tp:\n",
    "            dynamic_4th_R_shares = int(np.floor(\n",
    "                (dynamic_shares_total - dynamic_1st_R_shares - dynamic_2nd_R_shares - dynamic_3rd_R_shares) * \n",
    "                set_4th_R_percent / (100 - set_1st_R_percent - set_2nd_R_percent - set_3rd_R_percent))  )\n",
    "\n",
    "        # Setting shares to the default\n",
    "        timestamp_row['shares'] = dynamic_shares_total\n",
    "\n",
    "        # Putting the timestamp_row back into the original dataframe (doing each cell individually because it's much faster)\n",
    "        for col in df_1min.columns:\n",
    "            df_1min.at[timestamp, col] = timestamp_row[col]\n",
    "\n",
    "        \n",
    "        # Step 4: Forward filling the sl, be, and tp prices for the day (to save time and space)\n",
    "    \n",
    "        # Creating entry to eod dataframe (df_eeod) so we don't need to constantly use the cond_ent_EOD condition from before\n",
    "        df_eeod = df_1min.loc[(df_1min.index >= timestamp) & (df_1min.index.date == timestamp.date())].copy(deep = True)\n",
    "\n",
    "        # Forward filling entries, exits, and shares\n",
    "        df_eeod['sep_w_slippage'] = df_eeod['sep_w_slippage'].ffill()\n",
    "        df_eeod['BHOD_SL'] = df_eeod['BHOD_SL'].ffill()\n",
    "        df_eeod['BHOD_BE'] = df_eeod['BHOD_BE'].ffill()\n",
    "        df_eeod['TP_Long_1st_R_price'] = df_eeod['TP_Long_1st_R_price'].ffill()\n",
    "        df_eeod['TP_Long_2nd_R_price'] = df_eeod['TP_Long_2nd_R_price'].ffill()\n",
    "        df_eeod['TP_Long_3rd_R_price'] = df_eeod['TP_Long_3rd_R_price'].ffill()\n",
    "        df_eeod['TP_Long_4th_R_price'] = df_eeod['TP_Long_4th_R_price'].ffill()\n",
    "        df_eeod['shares'] = df_eeod['shares'].ffill()\n",
    "\n",
    "\n",
    "        # Step 5: Checking selling conditions and setting indicators\n",
    "    \n",
    "        # Setting conditions when price hits tp level\n",
    "        hit_TP_Long_1st_R_price_f = df_eeod['High'] >= df_eeod['TP_Long_1st_R_price']\n",
    "        hit_TP_Long_2nd_R_price_f = df_eeod['High'] >= df_eeod['TP_Long_2nd_R_price']\n",
    "        hit_TP_Long_3rd_R_price_f = df_eeod['High'] >= df_eeod['TP_Long_3rd_R_price']\n",
    "        hit_TP_Long_4th_R_price_f = df_eeod['High'] >= df_eeod['TP_Long_4th_R_price']\n",
    "\n",
    "        # If price hits any of the tp conditions, set those to True\n",
    "        df_eeod.loc[hit_TP_Long_1st_R_price_f, 'hit_1st_TP'] = True\n",
    "        df_eeod.loc[hit_TP_Long_2nd_R_price_f, 'hit_2nd_TP'] = True\n",
    "        df_eeod.loc[hit_TP_Long_3rd_R_price_f, 'hit_3rd_TP'] = True\n",
    "        df_eeod.loc[hit_TP_Long_4th_R_price_f, 'hit_4th_TP'] = True\n",
    "        \n",
    "        # Using the cumsum() function to get cumulative indicators\n",
    "        df_eeod['hit_1st_TP_cum'] = df_eeod['hit_1st_TP'].cumsum()\n",
    "        df_eeod['hit_2nd_TP_cum'] = df_eeod['hit_2nd_TP'].cumsum()\n",
    "        df_eeod['hit_3rd_TP_cum'] = df_eeod['hit_3rd_TP'].cumsum()\n",
    "        df_eeod['hit_4th_TP_cum'] = df_eeod['hit_4th_TP'].cumsum()\n",
    "\n",
    "        # If price falls below SL, set hit_SL to True\n",
    "        df_eeod.loc[df_eeod['Low'] < df_eeod['BHOD_SL'], 'hit_SL'] = True\n",
    "\n",
    "        # If price falls below BE, it's not the same CS as the entry, and XXX tp has been hit, set hit_BE to True\n",
    "        cond_low_below_be = df_eeod['Low'] < df_eeod['BHOD_BE']\n",
    "        cond_not_entry_CS = df_eeod.index != timestamp\n",
    "        cond_1st_tp_hit_true = move_SL_after_1st_tp & (df_eeod['hit_1st_TP_cum'] > 0)  # Do NOT get rid of these parentheses\n",
    "        cond_2nd_tp_hit_true = move_SL_after_2nd_tp & (df_eeod['hit_2nd_TP_cum'] > 0)\n",
    "        cond_3rd_tp_hit_true = move_SL_after_3rd_tp & (df_eeod['hit_3rd_TP_cum'] > 0)\n",
    "        cond_XXX_tp_hit = (cond_1st_tp_hit_true | cond_2nd_tp_hit_true | cond_3rd_tp_hit_true)\n",
    "        \n",
    "        df_eeod.loc[cond_low_below_be & cond_not_entry_CS & cond_XXX_tp_hit, 'hit_BE'] = True\n",
    "\n",
    "        # Getting cumulative indicators for stop loss and breakeven\n",
    "        df_eeod['hit_SL_cum'] = df_eeod['hit_SL'].cumsum()\n",
    "        df_eeod['hit_BE_cum'] = df_eeod['hit_BE'].cumsum()\n",
    "        \n",
    "\n",
    "        # Step 6: Removing the shares based on conditions and adding in total prices\n",
    "        \n",
    "        # Getting stop loss conditions (first time hitting the stop loss)\n",
    "        prev_hit_SL_cum = df_eeod['hit_SL_cum'].shift(1, fill_value = 0)  # Just checking for previous CS SL\n",
    "        prev_hit_SL_cum_is_0 = prev_hit_SL_cum == 0  # Prev CS did NOT hit SL\n",
    "        hit_SL_cum_is_1 = df_eeod['hit_SL_cum'] == 1  # Current cumulative SL is 1\n",
    "\n",
    "        cond_hit_SL = hit_SL_cum_is_1 & prev_hit_SL_cum_is_0  # First time hitting SL (SL indicator previously was 0 and current is 1)\n",
    "        cond_not_hit_SL = ~cond_hit_SL  # Not the first time hitting SL\n",
    "        \n",
    "        #####\n",
    "        \n",
    "        # Getting the 1st tp level exit amount\n",
    "        prev_hit_1st_TP_cum = df_eeod['hit_1st_TP_cum'].shift(1, fill_value=0)\n",
    "        hit_1st_TP_cum_is_1 = df_eeod['hit_1st_TP_cum'] == 1\n",
    "        prev_hit_1st_TP_cum_is_0 = prev_hit_1st_TP_cum == 0\n",
    "\n",
    "        cond_1st_tp_1 = hit_1st_TP_cum_is_1 & prev_hit_1st_TP_cum_is_0  # 1st time hitting 1st tp level\n",
    "        cond_1st_tp_f = cond_1st_tp_1 & cond_not_hit_SL  # \"Full\" condition 1\n",
    "\n",
    "        final_exit_amt_1st_tp = timestamp_row['TP_Long_1st_R_price'] * dynamic_1st_R_shares\n",
    "        df_eeod.loc[cond_1st_tp_f, 'exit_amt_1st_tp'] = final_exit_amt_1st_tp\n",
    "\n",
    "        #####\n",
    "        \n",
    "        # Getting the 2nd tp level exit amount\n",
    "        prev_hit_2nd_TP_cum = df_eeod['hit_2nd_TP_cum'].shift(1, fill_value = 0)\n",
    "        hit_2nd_TP_cum_is_1 = df_eeod['hit_2nd_TP_cum'] == 1\n",
    "        prev_hit_2nd_TP_cum_is_0 = prev_hit_2nd_TP_cum == 0\n",
    "\n",
    "        cond_2nd_tp_1 = hit_2nd_TP_cum_is_1 & prev_hit_2nd_TP_cum_is_0  # 1st time hitting 2nd tp level\n",
    "        cond_2nd_tp_2 = cond_not_hit_SL | (~cond_1st_tp_1 & move_SL_after_1st_tp)\n",
    "        # Originally, this is basically: \"Yes if NOT hitting sl AND FIRST 1st tp on this CS as well\"\n",
    "        #   ^This condition is to make sure you didn't hit the SL along with the 1st tp and 2nd tp levels together\n",
    "        #   ^If you've already hit the 1st tp previously, cond_1st_tp_1 will be False, making that part of the cond True\n",
    "        # The second \"&\" condition is to check if you have move_SL_after_1st_tp on; if you do, then cond_1st_tp_1 determines all\n",
    "        #   ^If move_SL_after_1st_tp is False, that means you're either moving the SL later or not moving the SL ever\n",
    "        #   ^In which case, the 2nd grouped cond is False, so just check if SL has been hit (making it the same as the 1st tp cond)\n",
    "        cond_2nd_tp = cond_2nd_tp_1 & cond_2nd_tp_2\n",
    "        \n",
    "        final_exit_amt_2nd_tp = timestamp_row['TP_Long_2nd_R_price'] * dynamic_2nd_R_shares\n",
    "        df_eeod.loc[cond_2nd_tp, 'exit_amt_2nd_tp'] = final_exit_amt_2nd_tp\n",
    "\n",
    "        #####\n",
    "        \n",
    "        # Getting the 3rd tp level exit amount\n",
    "        prev_hit_3rd_TP_cum = df_eeod['hit_3rd_TP_cum'].shift(1, fill_value = 0)\n",
    "        hit_3rd_TP_cum_is_1 = df_eeod['hit_3rd_TP_cum'] == 1\n",
    "        prev_hit_3rd_TP_cum_is_0 = prev_hit_3rd_TP_cum == 0\n",
    "\n",
    "        cond_3rd_tp_1 = hit_3rd_TP_cum_is_1 & prev_hit_3rd_TP_cum_is_0\n",
    "        cond_3rd_tp_2 = cond_2nd_tp_2 | (~cond_2nd_tp_1 & move_SL_after_2nd_tp)\n",
    "        # If you don't hit SL, then you're obviously fine. But if you do, depending on when you move SL, one of the cond will trigger:\n",
    "        # If move_SL_after_1st_tp is True, then this new grouped \"or\" condition is False, so just check if it's the 1st time you hit\n",
    "        #   ^the 1st tp level. If it is, even if cond_3rd_tp_1 is hit, it doesn't matter b/c SL takes priority.\n",
    "        # If move_SL_after_2nd_tp is True, then the previous grouped \"or\" condition is False, so now check if it's the 1st time you\n",
    "        #   ^hit the 2nd tp level. If it is, again, it doesn't matter b/c SL takes priority. Note in this scenario that it doesn't\n",
    "        #   ^matter whether you've hit the 1st tp. If you have previously, then whatever, SL is still in place. But even if you also\n",
    "        #   ^just hit the 1st tp along with the 2nd and 3rd, we already cover that with cond_2nd_tp_1, since in this case where you\n",
    "        #   ^haven't hit the 1st tp level previously, you necessarily will hit the 1st tp before you hit the second.\n",
    "        cond_3rd_tp = cond_3rd_tp_1 & cond_3rd_tp_2\n",
    "        \n",
    "        final_exit_amt_3rd_tp = timestamp_row['TP_Long_3rd_R_price'] * dynamic_3rd_R_shares\n",
    "        df_eeod.loc[cond_3rd_tp, 'exit_amt_3rd_tp'] = final_exit_amt_3rd_tp\n",
    "\n",
    "        #####\n",
    "\n",
    "        # Getting the 4th tp level exit amount\n",
    "        prev_hit_4th_TP_cum = df_eeod['hit_4th_TP_cum'].shift(1, fill_value = 0)\n",
    "        hit_4th_TP_cum_is_1 = df_eeod['hit_4th_TP_cum'] == 1\n",
    "        prev_hit_4th_TP_cum_is_0 = prev_hit_4th_TP_cum == 0\n",
    "\n",
    "        cond_4th_tp_1 = hit_4th_TP_cum_is_1 & prev_hit_4th_TP_cum_is_0\n",
    "        cond_4th_tp_2 = cond_3rd_tp_2 | (~cond_3rd_tp_1 & move_SL_after_3rd_tp)  # Same case as above\n",
    "        cond_4th_tp = cond_4th_tp_1 & cond_4th_tp_2\n",
    "        \n",
    "        final_exit_amt_4th_tp = timestamp_row['TP_Long_4th_R_price'] * dynamic_4th_R_shares\n",
    "        df_eeod.loc[cond_4th_tp, 'exit_amt_4th_tp'] = final_exit_amt_4th_tp\n",
    "\n",
    "        #####\n",
    "\n",
    "        # Condition to remove shares (rolling) if condition is met\n",
    "        cond_1st_TP_shares = df_eeod['hit_1st_TP_cum'] > 0\n",
    "        cond_2nd_TP_shares = df_eeod['hit_2nd_TP_cum'] > 0\n",
    "        cond_3rd_TP_shares = df_eeod['hit_3rd_TP_cum'] > 0\n",
    "        cond_4th_TP_shares = df_eeod['hit_4th_TP_cum'] > 0\n",
    "\n",
    "        # Removing shares if ### tp is hit (rolling)\n",
    "        df_eeod.loc[cond_1st_TP_shares, 'shares'] = (dynamic_shares_total - dynamic_1st_R_shares)\n",
    "        df_eeod.loc[cond_2nd_TP_shares, 'shares'] = (dynamic_shares_total - dynamic_1st_R_shares - dynamic_2nd_R_shares)\n",
    "        df_eeod.loc[cond_3rd_TP_shares, 'shares'] = (\n",
    "            dynamic_shares_total - dynamic_1st_R_shares - dynamic_2nd_R_shares - dynamic_3rd_R_shares)\n",
    "        df_eeod.loc[cond_4th_TP_shares, 'shares'] = (\n",
    "            dynamic_shares_total - dynamic_1st_R_shares - dynamic_2nd_R_shares - dynamic_3rd_R_shares - dynamic_4th_R_shares)\n",
    "\n",
    "        #####\n",
    "\n",
    "        # Putting SL condition here and using previous CS shares to get original share size before previous block of code changed it\n",
    "        can_move_SL_bc_1st_tp_not_hit = move_SL_after_1st_tp & prev_hit_1st_TP_cum_is_0  # Can move SL b/c 1st tp has not been hit\n",
    "        can_move_SL_bc_2nd_tp_not_hit = move_SL_after_2nd_tp & prev_hit_2nd_TP_cum_is_0\n",
    "        can_move_SL_bc_3rd_tp_not_hit = move_SL_after_3rd_tp & prev_hit_3rd_TP_cum_is_0\n",
    "        cond_sl = cond_hit_SL & (can_move_SL_bc_1st_tp_not_hit | can_move_SL_bc_2nd_tp_not_hit | can_move_SL_bc_3rd_tp_not_hit)\n",
    "        # Need this b/c SL ONLY triggers before/when XXX tp level condition is met, so you need to figure out when you move SL\n",
    "        #    ^The later you move SL, the more likely that cond_sl will be true (since there's more opportunities for SL to be hit)\n",
    "        # Btw, SL never triggers on the 1st CS\n",
    "\n",
    "        try:\n",
    "            # Getting final sl exit amount by multiplying the [higher of (SL - slippage) and the Low] and the remaining shares\n",
    "            sl_exit_price = np.maximum(timestamp_row['BHOD_SL'] - slippage, df_eeod.loc[cond_sl, 'Low'].iloc[0])\n",
    "            sl_prev_timestamp = df_eeod[cond_sl].index[0] - pd.Timedelta(minutes=1)  # Basically the time when SL hit minus one min\n",
    "            sl_exit_shares = df_eeod.loc[sl_prev_timestamp, 'shares']\n",
    "            df_eeod.loc[cond_sl, 'exit_amt_sl'] = sl_exit_price * sl_exit_shares\n",
    "        except: pass\n",
    "\n",
    "        #####\n",
    "\n",
    "        # Breakeven condition check (since it relies on the shares already calculated)\n",
    "        cond_hit_BE = df_eeod['hit_BE_cum'] == 1\n",
    "        prev_hit_BE_cum = df_eeod['hit_BE_cum'].shift(1, fill_value = 0)\n",
    "        prev_hit_BE_cum_is_0 = prev_hit_BE_cum == 0\n",
    "\n",
    "        cond_be_1 = cond_hit_BE & prev_hit_BE_cum_is_0\n",
    "        cond_be = cond_be_1 & cond_4th_tp_2\n",
    "        # Basically, SL trumps BE ONLY when they happen at the same time ALONG with whichever tp level you move SL\n",
    "        #   ^Btw, you don't need the [ (move_SL_after_XXX_tp & cond_XXX_tp_2 ) or ... ] because cond_4th_tp_2 already has conditions\n",
    "        #   ^baked into it for the 3 move_SL_after_XXX_tp levels. Essentially, this second part of the condition is used to make sure\n",
    "        #   ^that you don't hit SL and BE on the same CS, and cond_4th_tp_2 covers all of them.\n",
    "        # Another way to think about it is that it's essentially the same as the code below, but just faster:\n",
    "        # cond_be = cond_be_1 & ( (move_SL_after_1st_tp & cond_2nd_tp_2 ) | (move_SL_after_2nd_tp & cond_3rd_tp_2 )\n",
    "        #                       | (move_SL_after_3rd_tp & cond_4th_tp_2 ) )\n",
    "\n",
    "        try:\n",
    "            # Getting final be exit amount by multiplying the [higher of (BE - slippage) and the Low] and the remaining shares\n",
    "            be_exit_price = np.maximum(timestamp_row['BHOD_BE'] - slippage, df_eeod.loc[cond_be, 'Low'].iloc[0])\n",
    "            be_exit_shares = df_eeod.loc[cond_be, 'shares'].iloc[0]\n",
    "            df_eeod.loc[cond_be, 'exit_amt_be'] = be_exit_price * be_exit_shares\n",
    "        except: pass\n",
    "\n",
    "        #####\n",
    "\n",
    "        # Setting shares to 0 if either sl or be is hit (need to come after so we can do sl/be exit amount calculations first)\n",
    "        df_eeod.loc[(df_eeod['hit_SL_cum'] > 0) | (df_eeod['hit_BE_cum'] > 0), 'shares'] = 0.0\n",
    "\n",
    "        #####\n",
    "\n",
    "        # Getting out completely at 3:55pm at the Close price\n",
    "        exit_cond_3_55_pm = df_eeod.index.time == pd.to_datetime('15:55:00').time()\n",
    "\n",
    "        # Setting total exit amount at 3:55pm\n",
    "        eod_exit_price = df_eeod.loc[exit_cond_3_55_pm, 'Close'].iloc[0]\n",
    "        eod_exit_shares = df_eeod.loc[exit_cond_3_55_pm, 'shares'].iloc[0]\n",
    "        df_eeod.loc[(exit_cond_3_55_pm & (df_eeod['shares'] != 0)), 'exit_amt_3_55_pm'] = eod_exit_price * eod_exit_shares\n",
    "        df_eeod.loc[exit_cond_3_55_pm, 'shares'] = 0.0  # Setting shares to 0 at 3:55pm\n",
    "\n",
    "        \n",
    "        # Step 7: Filling in the trades table\n",
    "\n",
    "        # Getting the indices to get the correct exit amounts\n",
    "        cond_first_0_shares_ind = df_eeod['shares'] == 0\n",
    "        first_zero_shares_index = (cond_first_0_shares_ind).idxmax() if (cond_first_0_shares_ind).any() else None\n",
    "\n",
    "        # Getting the exit sums\n",
    "        ext_amts = df_eeod.loc[df_eeod.index <= first_zero_shares_index, ['exit_amt_1st_tp', 'exit_amt_2nd_tp', 'exit_amt_3rd_tp',\n",
    "            'exit_amt_4th_tp', 'exit_amt_sl', 'exit_amt_be', 'exit_amt_3_55_pm']].sum(axis = 0)\n",
    "\n",
    "        # Getting only the ones that had a transaction\n",
    "        ext_amts_true = ext_amts[ext_amts > 0]\n",
    "\n",
    "        # Matching the exit amount with their respective shares (assuming this tp level is reached)\n",
    "        tp_dict = {'exit_amt_1st_tp': dynamic_1st_R_shares, 'exit_amt_2nd_tp': dynamic_2nd_R_shares,\n",
    "            'exit_amt_3rd_tp': dynamic_3rd_R_shares, 'exit_amt_4th_tp': dynamic_4th_R_shares}\n",
    "        \n",
    "        # Getting the values for the trades table\n",
    "        for exit_type in ext_amts_true.index:\n",
    "\n",
    "            # If the exit hits a tp level\n",
    "            if exit_type in ['exit_amt_1st_tp', 'exit_amt_2nd_tp', 'exit_amt_3rd_tp', 'exit_amt_4th_tp']:\n",
    "                _Size = tp_dict[exit_type]\n",
    "                _ExitPrice = ext_amts_true[exit_type] / _Size\n",
    "\n",
    "            # If the exit is at sl\n",
    "            elif exit_type == 'exit_amt_sl':\n",
    "                _Size = sl_exit_shares\n",
    "                _ExitPrice = sl_exit_price\n",
    "\n",
    "            # If the exit is at be\n",
    "            elif exit_type == 'exit_amt_be':\n",
    "                _Size = be_exit_shares\n",
    "                _ExitPrice = be_exit_price\n",
    "\n",
    "            # If the exit is at 3:55pm\n",
    "            else:\n",
    "                _Size = eod_exit_shares\n",
    "                _ExitPrice = eod_exit_price\n",
    "\n",
    "            # Setting fee values\n",
    "            broker_fees = max(var_dict['Commission per Share ($)'] * _Size, var_dict['Min Comm per Order ($)'])\n",
    "            total_fees = broker_fees + _Size * (var_dict['Exchange Fees ($)'] + var_dict['Other Fees ($)'])\n",
    "            \n",
    "            # Setting remaining values\n",
    "            _EntryPrice = _sep_w_slippage\n",
    "            _PnL = (_ExitPrice - _EntryPrice) * _Size\n",
    "            _PnL_AF = _PnL - total_fees  # PnL After Fees                              \n",
    "            _EntryTime = timestamp\n",
    "            _ExitTime = df_eeod[df_eeod[exit_type] > 0].index[0]\n",
    "            _Duration = _ExitTime - _EntryTime\n",
    "            \n",
    "            # Filling in the trades table      \n",
    "            trades_table_dict = {\n",
    "                'Size': _Size,\n",
    "                'EntryPrice': _EntryPrice,\n",
    "                'ExitPrice': _ExitPrice,\n",
    "                'PnL': _PnL,\n",
    "                'PnL_AF': _PnL_AF,\n",
    "                'EntryTime': _EntryTime,\n",
    "                'ExitTime': _ExitTime,\n",
    "                'Duration': _Duration\n",
    "            }\n",
    "\n",
    "            # Setting indicators on whether or not to include the trade for that volume condition\n",
    "            for volume_condition_name in vol_conds_list:\n",
    "\n",
    "                # _EntryTime is greater than the most recent (last in list) and it's actually supposed to be an entry\n",
    "                if _EntryTime > recent_exits_dict[volume_condition_name][-1] and _EntryTime in vol_cond_dict[volume_condition_name]:\n",
    "                    trades_table_dict[volume_condition_name] = True  # Count this trade\n",
    "\n",
    "                    if exit_type == (ext_amts_true.index)[-1]:  # Appending timestamp onto recent exits if at end of exit_type list\n",
    "                        recent_exits_dict[volume_condition_name].append(first_zero_shares_index)\n",
    "\n",
    "                # Otherwise, do not count this trade\n",
    "                else:\n",
    "                    trades_table_dict[volume_condition_name] = False  # Don't count this trade\n",
    "\n",
    "            # Inputting into the trades table\n",
    "            trades_table.loc[len(trades_table)] = trades_table_dict\n",
    "\n",
    "        \n",
    "    # Part 5: Returning the final trades table\n",
    "    \n",
    "    return trades_table\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fddcdaa2-4a7c-4ae1-85f0-dceced687e67",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Data Analysis in Function Format\n",
    "We now define a few functions to analyze our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f010cb54-2f83-4eae-9813-e0cf88875493",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining function to produce data analysis results (trade_results need to come in the correct format (index goes from 0 to n))\n",
    "def metrics_table(trade_results, strategy_name, starting_cash = 100_000, show_drawdown_plot = False):  \n",
    "\n",
    "    # Getting the trade PnL for each full trade (not just partials)\n",
    "    trade_pnl = trade_results.groupby('EntryTime')['PnL'].sum()  # Group by EntryTime (our \"Trade ID\") and sum PnL for each trade\n",
    "    \n",
    "    # Net profit (with no fees and fees)\n",
    "    net_profit = trade_results['PnL'].sum()\n",
    "    net_profit_fees = trade_results['PnL_AF'].sum()\n",
    "\n",
    "    # Average share size\n",
    "    avg_share_size = trade_results.groupby('EntryTime')['Size'].sum().mean()\n",
    "\n",
    "    # Profit factor\n",
    "    pos_trades_dollar_total = trade_pnl[trade_pnl > 0].sum()\n",
    "    neg_trades_dollar_total = trade_pnl[trade_pnl <= 0].sum()\n",
    "    profit_factor = pos_trades_dollar_total / neg_trades_dollar_total * -1\n",
    "\n",
    "    # Win ratio\n",
    "    num_pos_trades = (trade_pnl > 0).sum()  # Count of full trades (not just partials) with positive PnL\n",
    "    num_neg_trades = (trade_pnl <= 0).sum()\n",
    "    num_trades_total = len(trade_pnl)  # Total number of trades\n",
    "    win_ratio = num_pos_trades / num_trades_total\n",
    "    \n",
    "    # Profit/loss per winning/losing trade\n",
    "    avg_winner_amount = trade_pnl[trade_pnl > 0].mean()\n",
    "    avg_loser_amount = trade_pnl[trade_pnl <= 0].mean()\n",
    "\n",
    "    # Risk:reward ratio\n",
    "    rr_ratio = avg_loser_amount / avg_winner_amount * -1\n",
    "\n",
    "    # Expected profitability\n",
    "    loss_ratio = 1 - win_ratio\n",
    "    expected_profitability = (win_ratio * avg_winner_amount) + (loss_ratio * avg_loser_amount)\n",
    "    \n",
    "    # Expected value (same result as expected profitability, but calculated differently)\n",
    "    expected_value = net_profit / num_trades_total\n",
    "\n",
    "    # Biggest winner / loser\n",
    "    biggest_winner = round(max(trade_pnl), 3)\n",
    "    biggest_loser = round(min(trade_pnl), 3)\n",
    "\n",
    "    # Equity peak and trough\n",
    "    cum_equity = trade_results['PnL'].cumsum()\n",
    "    equity_peak = max(cum_equity)\n",
    "    equity_trough = min(cum_equity)\n",
    "\n",
    "    # Maximum winning and losing streaks:\n",
    "    signs = trade_pnl.apply(lambda x: 1 if x > 0 else -1)  # Converting to binary indicators (1 for (+), -1 for (-) or (0))\n",
    "    streaks = signs.groupby((signs != signs.shift()).cumsum())  # Identifying consecutive streaks using groupby and cumsum\n",
    "    # Breaking the above code down: signs != signs.shift() gives True when the sign shifts, and cumsum() takes the sum of the Trues (1) and\n",
    "    # Falses (0), so when the sign shifts (i.e., another True (i.e., 1)), it gives a different number, and it groups by those same numbers\n",
    "    max_winning_streak = streaks.apply(lambda x: (x == 1).sum()).max()  # Applying a sum to the 1's in each group (streaks)\n",
    "    max_losing_streak = streaks.apply(lambda x: (x == -1).sum()).max()  # Applying a sum to the -1's in each group (streaks)\n",
    "\n",
    "    # Max drawdown\n",
    "    cum_pnl_series = trade_pnl.cumsum() + starting_cash  # Getting the cumulative pnl as a series\n",
    "    grouped_pnl_df = cum_pnl_series.to_frame(name = 'cum_pnl')  # Converting the cum_pnl_series to a df\n",
    "    grouped_pnl_df['running_max'] = grouped_pnl_df['cum_pnl'].cummax()  # Getting the running maximum (the running peak of equity)\n",
    "    grouped_pnl_df['drawdown'] = grouped_pnl_df['cum_pnl'] - grouped_pnl_df['running_max']  # Getting drawdown for each trade\n",
    "    max_drawdown = grouped_pnl_df['drawdown'].min()  # Getting the max drawdown (most negative amount)\n",
    "    max_drawdown_perc = (grouped_pnl_df['drawdown'] / grouped_pnl_df['running_max']).min() * 100  # Getting max drawdown as a percent\n",
    "\n",
    "    # Max drawdown plot (if show_drawdown_plot set to True) \n",
    "    if show_drawdown_plot:\n",
    "        # Plot cumulative PnL and running maximum\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.plot(grouped_pnl_df.index, grouped_pnl_df['cum_pnl'], label = 'Cumulative PnL', color = 'blue')\n",
    "        plt.plot(grouped_pnl_df.index, grouped_pnl_df['running_max'], label = 'Running Max', color = 'green')\n",
    "        \n",
    "        # Plot the drawdown as an area (below the cumulative PnL)\n",
    "        plt.fill_between(grouped_pnl_df.index, grouped_pnl_df['cum_pnl'], grouped_pnl_df['running_max'], \n",
    "                         where = (grouped_pnl_df['cum_pnl'] < grouped_pnl_df['running_max']),\n",
    "                         color = 'red', alpha = 0.3, label = 'Drawdown')\n",
    "        \n",
    "        # Add titles and labels\n",
    "        plt.title('Maximum Drawdown Visualization')\n",
    "        plt.xlabel('Date')\n",
    "        plt.ylabel('PnL')\n",
    "        \n",
    "        # Show the legend\n",
    "        plt.legend()\n",
    "        \n",
    "        # Display the plot\n",
    "        plt.show()\n",
    "\n",
    "    # Making trade_pnl into a dataframe for deeper analyses\n",
    "    full_trade_df = trade_pnl.to_frame(name = 'trade_pnl')\n",
    "\n",
    "    # Average $ used for trade\n",
    "    full_trade_df['total_trade_size'] = trade_results.groupby('EntryTime')['Size'].sum()\n",
    "    full_trade_df['entry_amount'] = trade_results.groupby('EntryTime')['EntryPrice'].mean()\n",
    "    full_trade_df['amount_used_for_trade'] = full_trade_df['total_trade_size'] * full_trade_df['entry_amount']\n",
    "    avg_amount_used_for_trade = full_trade_df['amount_used_for_trade'].mean()\n",
    "    \n",
    "    # Maximum $ used for a trade\n",
    "    max_amount_used_for_trade = full_trade_df['amount_used_for_trade'].max()\n",
    "\n",
    "    # Average holding time (from entry to last exit (partial))\n",
    "    last_holding_times = trade_results.groupby('EntryTime')['Duration'].nth(-1)  # Get the last Duration for each EntryTime\n",
    "    average_holding_time = last_holding_times.mean().total_seconds() / 60  # Average holding time in minutes\n",
    "\n",
    "    \n",
    "    # Average holding time for each partial\n",
    "\n",
    "    # Creating dataframe for partials analysis\n",
    "    partials_df = trade_results[['EntryTime', 'Duration', 'PnL']].copy()\n",
    "\n",
    "    # Assign a position (partial) number within each EntryTime\n",
    "    partials_df['Trade_Position'] = trade_results.groupby('EntryTime').cumcount() + 1\n",
    "\n",
    "    # Calculate mean holding time for each position (partial)\n",
    "    avg_hold_time_by_position = partials_df.groupby('Trade_Position')['Duration'].mean()\n",
    "\n",
    "    # Convert mean holding times to minutes\n",
    "    avg_hold_time_by_pos_min = avg_hold_time_by_position.apply(lambda x: x.total_seconds() / 60)\n",
    "    # This metric is not very useful to us, since it just takes every partial at a particular time and average it out\n",
    "    \n",
    "    # Average holding time for each partial number (making it so that we get the avg holding time depending on how many partials it took)\n",
    "    # Get the last Duration for each EntryBar along with its position number:\n",
    "    last_holding_times_w_pos = partials_df.groupby('EntryTime').apply(lambda x: x.iloc[-1])[['Duration', 'Trade_Position']]\n",
    "    avg_hold_time_by_trade_num = last_holding_times_w_pos.groupby('Trade_Position')['Duration'].mean()\n",
    "    avg_hold_time_by_trade_num_min = avg_hold_time_by_trade_num.apply(lambda x: x.total_seconds() / 60)\n",
    "    # This might be interesting; in this current test, the time it takes to get out after you do your first partial (2) is actually less\n",
    "    # than the time it takes to get out after your second partial (3) (10.2 min vs 6.4 min). This could indicate that after the first\n",
    "    # partial, if it takes too long, it could be could to just get out before the price drops to breakeven (could be worth doing more\n",
    "    # analysis to see when drop happens).\n",
    "\n",
    "    # Putting everything into a metrics table\n",
    "    metrics = {\n",
    "        'Strategy ID': strategy_name,\n",
    "        'Net Profit ($)': net_profit,\n",
    "        'Net Profit w Fees ($)': net_profit_fees,\n",
    "        'Avg Share Size': avg_share_size,\n",
    "        'Pos Trades Total ($)': pos_trades_dollar_total,\n",
    "        'Neg Trades Total ($)': neg_trades_dollar_total,\n",
    "        'Profit Factor': profit_factor,\n",
    "        '# Winning Trades': num_pos_trades,\n",
    "        '# Total Trades': num_trades_total,\n",
    "        'Win Ratio': win_ratio,\n",
    "        'R:R Ratio': rr_ratio,\n",
    "        'Expected Profitability (also EV) ($)': expected_profitability,\n",
    "        'Avg Winning Trade ($)': avg_winner_amount,\n",
    "        'Avg Losing Trade ($)': avg_loser_amount,\n",
    "        'Biggest Winner ($)': biggest_winner,\n",
    "        'Biggest Loser ($)': biggest_loser,\n",
    "        'Peak Equity ($)': equity_peak,\n",
    "        'Trough Equity ($)': equity_trough,\n",
    "        'Max Win Streak': max_winning_streak,\n",
    "        'Max Lose Streak': max_losing_streak,\n",
    "        'Max Drawdown (%)': max_drawdown_perc,\n",
    "        'Avg Amount Used for Trade ($)': avg_amount_used_for_trade,\n",
    "        'Max Amount Used for Trade ($)': max_amount_used_for_trade,\n",
    "        'Avg Holding Time (min)': average_holding_time\n",
    "    }\n",
    "\n",
    "    # Converting metrics to metrics_df to make things easier\n",
    "    metrics_df = pd.DataFrame([metrics])\n",
    "    metrics_df = metrics_df.round(3)  # Rounding the metrics table to 3 decimal places\n",
    "\n",
    "    return metrics_df\n",
    "\n",
    "    # The metrics below I've included for further analysis, but they're more difficult to implement and are not used right now\n",
    "    \n",
    "    # Sharpe ratio\n",
    "    # Supposed to measure volatility of your position, but it's quite difficult to calculate as we don't have the risk-free return nor the\n",
    "    # true return %s. We're just working with some arbitrarily large amount of cash, so it's hard to specify what our true return % is.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ce214ae9-5154-4fc4-a4bc-8062ecc93cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def partials_table_v2(trade_results):\n",
    "\n",
    "    # Creating dataframe for partials analysis\n",
    "    partials_df = trade_results[['EntryTime', 'PnL']].copy()\n",
    "\n",
    "    # Assign a position (partial) number within each EntryTime\n",
    "    partials_df['Trade_Position'] = trade_results.groupby('EntryTime').cumcount() + 1\n",
    "\n",
    "    # Creating a column with the total count trades taken for each unique EntryTime (will be useful later)\n",
    "    total_trades_taken = partials_df['EntryTime'].value_counts()  # Calculate the frequency of each EntryTime\n",
    "    partials_df['Total_Trades_Taken'] = partials_df['EntryTime'].map(total_trades_taken)  # Map the frequency count to a new column in df\n",
    "    \n",
    "    \n",
    "    # Table 1: Finding the additional amount gained by reaching each tp level\n",
    "\n",
    "    # Adding a new column to the previous df to include what the TRUE trade position is (e.g., met tp lvl 1, did not meet tp lvl 1, etc.)\n",
    "    partials_df['Trade_Position_True'] = None\n",
    "\n",
    "    # Getting a list of the unique trades taken (so 1 to the [# of partials])\n",
    "    types_of_partials = np.sort(total_trades_taken.unique())\n",
    "    \n",
    "    for i in types_of_partials:  # This condition just loops through each unique partial number (usually just [1, 2, 3])\n",
    "\n",
    "        # Assigning a positive number to represent trades that reached a tp and took a partial profit at that level\n",
    "        partials_df['Trade_Position_True'] = np.where(\n",
    "            (partials_df['Trade_Position'] == int(i)) & (partials_df['PnL'] > 0), float(i), partials_df['Trade_Position_True'])\n",
    "        \n",
    "        # Assigning a negative number to represent trades that didn't reach that tp level (also includes trades that ended (-) @ EOD close)\n",
    "        partials_df['Trade_Position_True'] = np.where(\n",
    "            (partials_df['Trade_Position'] == int(i)) & (partials_df['PnL'] <= 0), -1.0 * float(i), partials_df['Trade_Position_True'])\n",
    "    \n",
    "    for i in types_of_partials[:-1]:  # This condition loops through all partials except the last partial\n",
    "\n",
    "        # Assigning (i - 0.5) to represent the trades that were closed out because of EOD, but ended positive before [i] tp level\n",
    "        partials_df['Trade_Position_True'] = np.where((partials_df['Trade_Position_True'] == i) & (\n",
    "            partials_df['Total_Trades_Taken'] == i), float(i) - 0.5, partials_df['Trade_Position_True'])\n",
    "        \n",
    "        # For some clarity, this checks whether the trade was (-) or (+) from Trade_Position_True; if it were positive, and it only took\n",
    "        # [i] number of trades, that means it MUST have gotten stopped out at the EOD, and it stopped positively (since normally a (+)\n",
    "        # Trade_Position_True would indicate that the trade would move onto the next partial, but since there were only [i] number of\n",
    "        # trades, that can't have happened)\n",
    "        \n",
    "    # Making sure the order of the dataframe is as it's supposed to be (this is the sole purpose of Trade_Position_Adjusted)\n",
    "    partials_df['Trade_Position_Adjusted'] = partials_df['Trade_Position_True'].apply(  # First creating a column with correct ordering...\n",
    "        lambda x: abs(x) - 0.9 if x < 0 else x)  # ...by making it so that negative numbers are turned into a value of (tp level - 0.9)\n",
    "    partials_df = partials_df.sort_values('Trade_Position_Adjusted').reset_index()  # Sort by more appropriate order\n",
    "\n",
    "    # Getting the frequency of each tp partial status (using Trade_Position_Adjusted)\n",
    "    frequency_of_trade = partials_df['Trade_Position_Adjusted'].value_counts().sort_index()\n",
    "\n",
    "    # Creating our final gain_across_tp_levels_df dataframe\n",
    "    gain_across_tp_levels_df = frequency_of_trade.to_frame(name = 'Frequency').reset_index()\n",
    "\n",
    "    # Defining a function to apply the conditions below and convert Trade_Position_Adjusted floats to more clear string values\n",
    "    def convert_float_to_string(value):\n",
    "        integer_part = int(value)  # Get the integer part\n",
    "        decimal_part = round(value - integer_part, 1)  # Get the decimal part (rounded to avoid float precision issues)\n",
    "        \n",
    "        # Apply conditions\n",
    "        if decimal_part == 0.1:\n",
    "            return f\"Didn't reach tp level {integer_part + 1}\"\n",
    "        elif decimal_part == 0.5:\n",
    "            return f\"EOD close before tp level {integer_part + 1}\"\n",
    "        elif decimal_part == 0.0:\n",
    "            return f\"Hit tp level {integer_part}\"\n",
    "        else:\n",
    "            return value  # Leave as is if it doesn't match any conditions\n",
    "\n",
    "    # Applying the function back to a new Trade Status column\n",
    "    gain_across_tp_levels_df['Trade Status'] = gain_across_tp_levels_df['Trade_Position_Adjusted'].apply(convert_float_to_string)\n",
    "    gain_across_tp_levels_df = gain_across_tp_levels_df[['Trade_Position_Adjusted', 'Trade Status', 'Frequency']]  # Reording columns\n",
    "    gain_across_tp_levels_df = gain_across_tp_levels_df.set_index('Trade_Position_Adjusted')  # To make things easier in the next steps\n",
    "\n",
    "    # Getting remaining metrics associated with each trade partial\n",
    "    avg_gain_at_tp_level = partials_df.groupby(\n",
    "        'Trade_Position_Adjusted', observed = True)['PnL'].mean().sort_index()  # Average gain at each tp level\n",
    "    total_gain_at_tp_level = partials_df.groupby(\n",
    "        'Trade_Position_Adjusted', observed = True)['PnL'].sum().sort_index()  # Total gain at tp level\n",
    "    # Re-sorting partials_df for easier viewing (and potential analyses down the line)\n",
    "    partials_df = partials_df.sort_values('EntryTime').reset_index(drop = True).drop(columns=['index'])\n",
    "\n",
    "    # Combining both into the gain_across_tp_levels_df dataframe\n",
    "    gain_across_tp_levels_df['Avg Gain at TP Level'] = avg_gain_at_tp_level\n",
    "    gain_across_tp_levels_df['Total Gain at TP Level'] = total_gain_at_tp_level\n",
    "    \n",
    "\n",
    "    # Table 2: Number of trades reached (i.e., the number of trades that took place when the SL/BE was hit)\n",
    "\n",
    "    # Getting the trade PnL for each full trade (not just partials)\n",
    "    trade_pnl = trade_results.groupby('EntryTime')['PnL'].sum()  # Group by EntryTime (our \"Trade ID\") and sum PnL for each trade\n",
    "    full_trade_df = trade_pnl.to_frame(name = 'trade_pnl')  # Making trade_pnl into a dataframe for deeper analyses\n",
    "\n",
    "    # Count occurrences of each unique trade ID (EntryTime) and check if the last PnL is positive\n",
    "    def label_occurrence(trade_group):\n",
    "        last_partial_num = int(types_of_partials[-1])\n",
    "        count = len(trade_group)  # Number of occurrences for the EntryTime\n",
    "        last_pnl_positive = trade_group['PnL'].iloc[-1] > 0  # Check if last PnL is positive\n",
    "        # Label as \"[last_partial_num]tp\" if last partial and the last PnL is (+) (so the last tp level reached); otherwise just the count\n",
    "        return f\"{count}tp\" if count == last_partial_num and last_pnl_positive else str(count)\n",
    "    \n",
    "    # Apply the labeling function to each EntryTime group\n",
    "    full_trade_df['num_trades_at_sl_be'] = trade_results.groupby('EntryTime').apply(label_occurrence)\n",
    "    \n",
    "    # Getting metrics associated with the pnl per trade\n",
    "    frequency_of_counts = full_trade_df['num_trades_at_sl_be'].value_counts().sort_index()  # Count frequency of each labeled occurrence\n",
    "    avg_pnl_per_trade = full_trade_df.groupby('num_trades_at_sl_be')['trade_pnl'].mean().sort_index()  # Find avg PnL at each partial\n",
    "    total_pnl_at_stage = full_trade_df.groupby('num_trades_at_sl_be')['trade_pnl'].sum().sort_index()  # Find tot. PnL @ each partial stage\n",
    "\n",
    "    # Combining the series into one dataframe for easier viewing\n",
    "    num_trades_at_sl_be_df = pd.concat([frequency_of_counts, avg_pnl_per_trade, total_pnl_at_stage], axis = 1)\n",
    "    num_trades_at_sl_be_df = num_trades_at_sl_be_df.reset_index()\n",
    "    num_trades_at_sl_be_df.columns = ['# of Trades at SL/BE', 'Frequency', 'Avg PnL per Trade', 'Total PnL at Stage']\n",
    "\n",
    "    # Returns a tuple of the two tables\n",
    "    return gain_across_tp_levels_df, num_trades_at_sl_be_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "015668dd-8615-4f5d-af71-235344634770",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trade_timing_PnL_graph(trade_results, timeframe = 'hour'):\n",
    "    \n",
    "    # Getting the trade PnL for each full trade (not just partials)\n",
    "    trade_pnl = trade_results.groupby('EntryTime')['PnL'].sum()  # Group by EntryTime (our \"Trade ID\") and sum PnL for each trade\n",
    "\n",
    "    # Setting figure size\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    # Getting graph for different timeframes\n",
    "    if timeframe == 'hour':\n",
    "\n",
    "        # Grouping trade pnl by hour\n",
    "        trade_pnl_hour = trade_pnl.groupby(trade_pnl.index.hour).mean()\n",
    "        trade_pnl_by_time = trade_pnl_hour\n",
    "    \n",
    "        # Plot cumulative PnL at different hours\n",
    "        plt.plot(trade_pnl_hour.index, trade_pnl_hour, label = 'trade_pnl_hour', color = 'blue')\n",
    "        \n",
    "        # Add xlabel and title\n",
    "        plt.xlabel('Entry Hour')\n",
    "        plt.title('Trade PnL by Hour')\n",
    "\n",
    "    elif timeframe == 'minute':\n",
    "\n",
    "        # Grouping trade pnl by minute\n",
    "        trade_pnl_minute = trade_pnl.groupby(trade_pnl.index.time).mean()\n",
    "\n",
    "        # Getting the full time index for graph\n",
    "        full_time_index = pd.date_range(start=\"09:30\", end=\"16:01\", freq=\"T\").time\n",
    "        \n",
    "        # Reindex the Series to include all times\n",
    "        trade_pnl_minute = trade_pnl_minute.reindex(full_time_index).fillna(0)\n",
    "        trade_pnl_by_time = trade_pnl_minute\n",
    "\n",
    "        # Convert time to datetime using a reference date (e.g., 2000-01-01)\n",
    "        reference_date = datetime(2000, 1, 1)\n",
    "        date_time_index = [datetime.combine(reference_date, t) for t in trade_pnl_minute.index]\n",
    "\n",
    "        # Plot cumulative PnL at different times\n",
    "        plt.scatter(date_time_index, trade_pnl_minute, label = 'trade_pnl_minute', color = 'blue', s = 10)\n",
    "\n",
    "        # Format the x-axis to show only time, with ticks every 30 minutes\n",
    "        ax = plt.gca()\n",
    "        ax.xaxis.set_major_locator(mdates.MinuteLocator(interval=30))  # Major ticks every 30 minutes\n",
    "        ax.xaxis.set_major_formatter(mdates.DateFormatter('%H:%M'))   # Format as HH:MM\n",
    "        \n",
    "        # Add xlabel and title\n",
    "        plt.xlabel('Entry Time')\n",
    "        plt.title('Trade PnL by Minute (Time)')\n",
    "        \n",
    "    # Adding in additional labels + legend and displaying the plot\n",
    "    plt.ylabel('Average PnL ($)')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    # Return the trade pnl by time (hour or minute)\n",
    "    return trade_pnl_by_time\n",
    "\n",
    "# trade_timing_PnL_graph(combined_results_MSFT_3y_v_f, timeframe = 'minute')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d68232f-a150-4540-a37a-4941b87980c7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Metrics Table Scripting\n",
    "We've defined some functions we can use above to analyze our results. However, to make it so that we can automatically analyze different volume thresholds without having to re-run the backtest, we can create another function that automatically takes each volume threshold and checks whether or not it needs to be added to the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "aad63a11-ef16-4f02-a457-6204f5c3e106",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a function to get the overall metrics table (for varying volume) from dask df\n",
    "def dask_to_var_vol_metrics(dask_df_1min, strategy, var_dict, vol_start, vol_end, vol_step):\n",
    "\n",
    "    # Getting a list of volume percents to cycle through\n",
    "    volume_perc_list = [i for i in range(vol_start, vol_end, vol_step)]\n",
    "    var_dict['Volume Percent List (%)'] = volume_perc_list\n",
    "\n",
    "    # Getting the initial final dataframe\n",
    "    metrics_table_full = pd.DataFrame()\n",
    "\n",
    "    # Splitting up dataset into partitions\n",
    "    partitions_v = dask_df_1min.to_delayed()\n",
    "\n",
    "    # Apply the function to each partition and get delayed objects for each backtest\n",
    "    results_v = [run_vec_backtest_on_part(partition, strategy, var_dict) for partition in partitions_v]\n",
    "\n",
    "    # Compute all results and combine the \"results\" dataframes into one dataframe using pd.concat\n",
    "    all_results_v = dask.compute(*results_v)\n",
    "    all_results_v_filtered = [df for df in all_results_v if not df.empty]  # Making sure there's no NA columns to get rid of warning\n",
    "    combined_results_v = pd.concat(all_results_v_filtered, axis=0).reset_index(drop = True)\n",
    "\n",
    "    # Writing up script to test different metrics (e.g., volumes) with the partitions in the full dataset\n",
    "    for i, volume_perc in enumerate(volume_perc_list):\n",
    "        \n",
    "        strat_name = 'BHOD Volume Threshold: ' + str(volume_perc) + '%'  # Naming the strategy\n",
    "        vol_perc_col_indic = combined_results_v.iloc[:, 8+i]  # Getting the indicator column to include that entry or not\n",
    "        combined_results_one_vol = combined_results_v[vol_perc_col_indic].reset_index(drop = True)  # Only keeping entries for that vol\n",
    "    \n",
    "        # Calculating metrics table for one specific entry condition, and then combining that metrics table to the overall metrics table\n",
    "        metrics_table_curr = metrics_table(combined_results_one_vol, strat_name, show_drawdown_plot = False)\n",
    "        metrics_table_full = pd.concat((metrics_table_full, metrics_table_curr), axis=0).reset_index(drop = True)\n",
    "        \n",
    "    return metrics_table_full\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43611aa9-4c71-433c-a0a6-0cf9b57be143",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Running Vectorized Backtest Across 19 Different Volume Thresholds\n",
    "Now that we have everything we need, we can run our vectorized BHOD backtest across multiple different volume thresholds (which is simply the volume required for an entry). We start by loading in the data, and then we run our backtest and calculate the metrics associated with the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8e4f0fe7-519f-4994-b5c7-781f1405a6b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing with dask MSFT 3y data\n",
    "MSFT_1min_poly_3y_v_f = dd.read_csv('../../../MSFT_1min_poly_3y_9am_4pm.csv', parse_dates=['timestamp'])\n",
    "MSFT_1min_poly_3y_v_f = MSFT_1min_poly_3y_v_f.set_index('timestamp')\n",
    "# MSFT_1min_poly_3y_v_f = MSFT_1min_poly_3y_v_f.loc['2024-07-15 00:00':'2024-07-30 00:00']\n",
    "MSFT_1min_poly_3y_v_f = MSFT_1min_poly_3y_v_f.reset_index()\n",
    "MSFT_1min_poly_3y_v_f['timestamp'] = MSFT_1min_poly_3y_v_f['timestamp'].dt.floor('S')\n",
    "MSFT_1min_poly_3y_v_f = MSFT_1min_poly_3y_v_f.set_index('timestamp')\n",
    "\n",
    "# Repartitioning so that each partition holds one day's worth of data (it does include weekends, but not a big deal)\n",
    "MSFT_1min_poly_3y_v_f = MSFT_1min_poly_3y_v_f.repartition(freq='10D')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b892f1d1-3bc0-4dc3-be22-a2cc4ea3dd37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30.470995903015137\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "metrics_table_test = dask_to_var_vol_metrics(MSFT_1min_poly_3y_v_f, BHOD_Full_Vec_v1, variables_dict, 10, 101, 5)\n",
    "print(time.time() - start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fdab5f0-a35b-4a88-9bc9-6e1772b1433a",
   "metadata": {},
   "source": [
    "For the event-driven backtest, it took over 60 seconds to run only a single volume threshold. Using this vectorized backtest, even with 19 times the number of volume thresholds to test, it took less than half the time. To make sure MSFT wasn't just a fluke, we can test this with TSLA as well and show what our final metrics table looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d4ddf414-373f-442e-ac32-60d9892df355",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing with dask TSLA 3y data\n",
    "TSLA_1min_poly_3y_v_f = dd.read_csv('../../../TSLA_1min_poly_3y_9am_4pm.csv', parse_dates=['timestamp'])\n",
    "TSLA_1min_poly_3y_v_f = TSLA_1min_poly_3y_v_f.set_index('timestamp')\n",
    "TSLA_1min_poly_3y_v_f = TSLA_1min_poly_3y_v_f.reset_index()\n",
    "TSLA_1min_poly_3y_v_f['timestamp'] = TSLA_1min_poly_3y_v_f['timestamp'].dt.floor('S')\n",
    "TSLA_1min_poly_3y_v_f = TSLA_1min_poly_3y_v_f.set_index('timestamp')\n",
    "\n",
    "# Repartitioning so that each partition holds one day's worth of data (it does include weekends, but not a big deal)\n",
    "TSLA_1min_poly_3y_v_f = TSLA_1min_poly_3y_v_f.repartition(freq='10D')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "686caad0-4c02-4c69-a274-9dcaeab7500d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40.05021595954895\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "metrics_table_test_TSLA = dask_to_var_vol_metrics(TSLA_1min_poly_3y_v_f, BHOD_Full_Vec_v1, variables_dict, 10, 101, 5)\n",
    "print(time.time() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "63f751f6-e2c5-4f69-a5c2-0f8da0688318",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Strategy ID</th>\n",
       "      <th>Net Profit ($)</th>\n",
       "      <th>Net Profit w Fees ($)</th>\n",
       "      <th>Avg Share Size</th>\n",
       "      <th>Pos Trades Total ($)</th>\n",
       "      <th>Neg Trades Total ($)</th>\n",
       "      <th>Profit Factor</th>\n",
       "      <th># Winning Trades</th>\n",
       "      <th># Total Trades</th>\n",
       "      <th>Win Ratio</th>\n",
       "      <th>...</th>\n",
       "      <th>Biggest Winner ($)</th>\n",
       "      <th>Biggest Loser ($)</th>\n",
       "      <th>Peak Equity ($)</th>\n",
       "      <th>Trough Equity ($)</th>\n",
       "      <th>Max Win Streak</th>\n",
       "      <th>Max Lose Streak</th>\n",
       "      <th>Max Drawdown (%)</th>\n",
       "      <th>Avg Amount Used for Trade ($)</th>\n",
       "      <th>Max Amount Used for Trade ($)</th>\n",
       "      <th>Avg Holding Time (min)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BHOD Volume Threshold: 10%</td>\n",
       "      <td>3166.620</td>\n",
       "      <td>1646.749</td>\n",
       "      <td>42.447</td>\n",
       "      <td>20201.290</td>\n",
       "      <td>-17034.670</td>\n",
       "      <td>1.186</td>\n",
       "      <td>1092</td>\n",
       "      <td>1920</td>\n",
       "      <td>0.569</td>\n",
       "      <td>...</td>\n",
       "      <td>48.9</td>\n",
       "      <td>-27.50</td>\n",
       "      <td>3689.935</td>\n",
       "      <td>-44.225</td>\n",
       "      <td>20</td>\n",
       "      <td>8</td>\n",
       "      <td>-0.954</td>\n",
       "      <td>9653.463</td>\n",
       "      <td>46670.85</td>\n",
       "      <td>8.580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BHOD Volume Threshold: 15%</td>\n",
       "      <td>3607.510</td>\n",
       "      <td>2123.290</td>\n",
       "      <td>42.330</td>\n",
       "      <td>19947.835</td>\n",
       "      <td>-16340.325</td>\n",
       "      <td>1.221</td>\n",
       "      <td>1073</td>\n",
       "      <td>1868</td>\n",
       "      <td>0.574</td>\n",
       "      <td>...</td>\n",
       "      <td>48.9</td>\n",
       "      <td>-27.50</td>\n",
       "      <td>3783.390</td>\n",
       "      <td>-23.825</td>\n",
       "      <td>19</td>\n",
       "      <td>8</td>\n",
       "      <td>-0.768</td>\n",
       "      <td>9605.352</td>\n",
       "      <td>46670.85</td>\n",
       "      <td>8.602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BHOD Volume Threshold: 20%</td>\n",
       "      <td>4073.300</td>\n",
       "      <td>2641.482</td>\n",
       "      <td>42.511</td>\n",
       "      <td>19538.535</td>\n",
       "      <td>-15465.235</td>\n",
       "      <td>1.263</td>\n",
       "      <td>1040</td>\n",
       "      <td>1793</td>\n",
       "      <td>0.580</td>\n",
       "      <td>...</td>\n",
       "      <td>48.9</td>\n",
       "      <td>-27.50</td>\n",
       "      <td>4257.785</td>\n",
       "      <td>-23.825</td>\n",
       "      <td>19</td>\n",
       "      <td>7</td>\n",
       "      <td>-0.653</td>\n",
       "      <td>9632.354</td>\n",
       "      <td>46670.85</td>\n",
       "      <td>8.677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>BHOD Volume Threshold: 25%</td>\n",
       "      <td>4426.755</td>\n",
       "      <td>3050.996</td>\n",
       "      <td>42.302</td>\n",
       "      <td>19100.140</td>\n",
       "      <td>-14673.385</td>\n",
       "      <td>1.302</td>\n",
       "      <td>1003</td>\n",
       "      <td>1718</td>\n",
       "      <td>0.584</td>\n",
       "      <td>...</td>\n",
       "      <td>48.9</td>\n",
       "      <td>-27.50</td>\n",
       "      <td>4594.330</td>\n",
       "      <td>-3.800</td>\n",
       "      <td>21</td>\n",
       "      <td>7</td>\n",
       "      <td>-0.703</td>\n",
       "      <td>9603.056</td>\n",
       "      <td>46670.85</td>\n",
       "      <td>8.884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>BHOD Volume Threshold: 30%</td>\n",
       "      <td>4976.085</td>\n",
       "      <td>3660.929</td>\n",
       "      <td>42.370</td>\n",
       "      <td>18472.940</td>\n",
       "      <td>-13496.855</td>\n",
       "      <td>1.369</td>\n",
       "      <td>971</td>\n",
       "      <td>1629</td>\n",
       "      <td>0.596</td>\n",
       "      <td>...</td>\n",
       "      <td>48.9</td>\n",
       "      <td>-27.50</td>\n",
       "      <td>5081.060</td>\n",
       "      <td>3.260</td>\n",
       "      <td>19</td>\n",
       "      <td>6</td>\n",
       "      <td>-0.593</td>\n",
       "      <td>9646.804</td>\n",
       "      <td>46670.85</td>\n",
       "      <td>9.037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>BHOD Volume Threshold: 35%</td>\n",
       "      <td>5166.500</td>\n",
       "      <td>3901.894</td>\n",
       "      <td>42.301</td>\n",
       "      <td>17909.065</td>\n",
       "      <td>-12742.565</td>\n",
       "      <td>1.405</td>\n",
       "      <td>938</td>\n",
       "      <td>1560</td>\n",
       "      <td>0.601</td>\n",
       "      <td>...</td>\n",
       "      <td>46.8</td>\n",
       "      <td>-27.50</td>\n",
       "      <td>5275.245</td>\n",
       "      <td>3.260</td>\n",
       "      <td>18</td>\n",
       "      <td>9</td>\n",
       "      <td>-0.426</td>\n",
       "      <td>9658.343</td>\n",
       "      <td>46670.85</td>\n",
       "      <td>9.113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>BHOD Volume Threshold: 40%</td>\n",
       "      <td>5549.320</td>\n",
       "      <td>4343.570</td>\n",
       "      <td>42.376</td>\n",
       "      <td>17271.580</td>\n",
       "      <td>-11722.260</td>\n",
       "      <td>1.473</td>\n",
       "      <td>904</td>\n",
       "      <td>1477</td>\n",
       "      <td>0.612</td>\n",
       "      <td>...</td>\n",
       "      <td>46.8</td>\n",
       "      <td>-27.50</td>\n",
       "      <td>5701.850</td>\n",
       "      <td>3.260</td>\n",
       "      <td>17</td>\n",
       "      <td>8</td>\n",
       "      <td>-0.397</td>\n",
       "      <td>9694.924</td>\n",
       "      <td>46670.85</td>\n",
       "      <td>9.227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>BHOD Volume Threshold: 45%</td>\n",
       "      <td>5918.890</td>\n",
       "      <td>4782.248</td>\n",
       "      <td>42.481</td>\n",
       "      <td>16697.200</td>\n",
       "      <td>-10778.310</td>\n",
       "      <td>1.549</td>\n",
       "      <td>855</td>\n",
       "      <td>1383</td>\n",
       "      <td>0.618</td>\n",
       "      <td>...</td>\n",
       "      <td>46.8</td>\n",
       "      <td>-27.50</td>\n",
       "      <td>5997.960</td>\n",
       "      <td>3.260</td>\n",
       "      <td>17</td>\n",
       "      <td>7</td>\n",
       "      <td>-0.368</td>\n",
       "      <td>9711.040</td>\n",
       "      <td>45239.16</td>\n",
       "      <td>9.386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>BHOD Volume Threshold: 50%</td>\n",
       "      <td>5909.005</td>\n",
       "      <td>4827.628</td>\n",
       "      <td>42.721</td>\n",
       "      <td>16089.085</td>\n",
       "      <td>-10180.080</td>\n",
       "      <td>1.580</td>\n",
       "      <td>811</td>\n",
       "      <td>1310</td>\n",
       "      <td>0.619</td>\n",
       "      <td>...</td>\n",
       "      <td>46.8</td>\n",
       "      <td>-27.50</td>\n",
       "      <td>5994.645</td>\n",
       "      <td>3.260</td>\n",
       "      <td>18</td>\n",
       "      <td>7</td>\n",
       "      <td>-0.330</td>\n",
       "      <td>9772.938</td>\n",
       "      <td>45239.16</td>\n",
       "      <td>9.555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>BHOD Volume Threshold: 55%</td>\n",
       "      <td>5702.845</td>\n",
       "      <td>4700.207</td>\n",
       "      <td>42.409</td>\n",
       "      <td>15049.415</td>\n",
       "      <td>-9346.570</td>\n",
       "      <td>1.610</td>\n",
       "      <td>754</td>\n",
       "      <td>1212</td>\n",
       "      <td>0.622</td>\n",
       "      <td>...</td>\n",
       "      <td>46.8</td>\n",
       "      <td>-27.50</td>\n",
       "      <td>5801.385</td>\n",
       "      <td>3.260</td>\n",
       "      <td>19</td>\n",
       "      <td>7</td>\n",
       "      <td>-0.356</td>\n",
       "      <td>9737.143</td>\n",
       "      <td>45239.16</td>\n",
       "      <td>9.806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>BHOD Volume Threshold: 60%</td>\n",
       "      <td>5539.180</td>\n",
       "      <td>4590.225</td>\n",
       "      <td>42.757</td>\n",
       "      <td>14263.845</td>\n",
       "      <td>-8724.665</td>\n",
       "      <td>1.635</td>\n",
       "      <td>713</td>\n",
       "      <td>1141</td>\n",
       "      <td>0.625</td>\n",
       "      <td>...</td>\n",
       "      <td>46.8</td>\n",
       "      <td>-27.50</td>\n",
       "      <td>5616.520</td>\n",
       "      <td>3.260</td>\n",
       "      <td>19</td>\n",
       "      <td>7</td>\n",
       "      <td>-0.300</td>\n",
       "      <td>9809.781</td>\n",
       "      <td>45239.16</td>\n",
       "      <td>9.835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>BHOD Volume Threshold: 65%</td>\n",
       "      <td>5688.460</td>\n",
       "      <td>4788.778</td>\n",
       "      <td>42.891</td>\n",
       "      <td>13792.225</td>\n",
       "      <td>-8103.765</td>\n",
       "      <td>1.702</td>\n",
       "      <td>677</td>\n",
       "      <td>1075</td>\n",
       "      <td>0.630</td>\n",
       "      <td>...</td>\n",
       "      <td>46.8</td>\n",
       "      <td>-27.50</td>\n",
       "      <td>5775.275</td>\n",
       "      <td>3.260</td>\n",
       "      <td>19</td>\n",
       "      <td>7</td>\n",
       "      <td>-0.263</td>\n",
       "      <td>9845.510</td>\n",
       "      <td>45239.16</td>\n",
       "      <td>10.082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>BHOD Volume Threshold: 70%</td>\n",
       "      <td>5767.495</td>\n",
       "      <td>4929.504</td>\n",
       "      <td>42.791</td>\n",
       "      <td>13062.090</td>\n",
       "      <td>-7294.595</td>\n",
       "      <td>1.791</td>\n",
       "      <td>637</td>\n",
       "      <td>996</td>\n",
       "      <td>0.640</td>\n",
       "      <td>...</td>\n",
       "      <td>46.8</td>\n",
       "      <td>-27.50</td>\n",
       "      <td>5858.990</td>\n",
       "      <td>3.260</td>\n",
       "      <td>23</td>\n",
       "      <td>7</td>\n",
       "      <td>-0.226</td>\n",
       "      <td>9830.087</td>\n",
       "      <td>45239.16</td>\n",
       "      <td>10.226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>BHOD Volume Threshold: 75%</td>\n",
       "      <td>5540.440</td>\n",
       "      <td>4767.350</td>\n",
       "      <td>42.890</td>\n",
       "      <td>12119.420</td>\n",
       "      <td>-6578.980</td>\n",
       "      <td>1.842</td>\n",
       "      <td>591</td>\n",
       "      <td>915</td>\n",
       "      <td>0.646</td>\n",
       "      <td>...</td>\n",
       "      <td>46.8</td>\n",
       "      <td>-24.48</td>\n",
       "      <td>5590.035</td>\n",
       "      <td>3.260</td>\n",
       "      <td>20</td>\n",
       "      <td>7</td>\n",
       "      <td>-0.230</td>\n",
       "      <td>9915.372</td>\n",
       "      <td>45239.16</td>\n",
       "      <td>10.462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>BHOD Volume Threshold: 80%</td>\n",
       "      <td>5203.485</td>\n",
       "      <td>4483.423</td>\n",
       "      <td>42.524</td>\n",
       "      <td>11390.750</td>\n",
       "      <td>-6187.265</td>\n",
       "      <td>1.841</td>\n",
       "      <td>550</td>\n",
       "      <td>855</td>\n",
       "      <td>0.643</td>\n",
       "      <td>...</td>\n",
       "      <td>46.8</td>\n",
       "      <td>-24.48</td>\n",
       "      <td>5283.680</td>\n",
       "      <td>3.260</td>\n",
       "      <td>19</td>\n",
       "      <td>7</td>\n",
       "      <td>-0.295</td>\n",
       "      <td>9876.176</td>\n",
       "      <td>45239.16</td>\n",
       "      <td>10.690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>BHOD Volume Threshold: 85%</td>\n",
       "      <td>5191.860</td>\n",
       "      <td>4519.580</td>\n",
       "      <td>41.974</td>\n",
       "      <td>10837.995</td>\n",
       "      <td>-5646.135</td>\n",
       "      <td>1.920</td>\n",
       "      <td>518</td>\n",
       "      <td>796</td>\n",
       "      <td>0.651</td>\n",
       "      <td>...</td>\n",
       "      <td>46.8</td>\n",
       "      <td>-24.48</td>\n",
       "      <td>5287.560</td>\n",
       "      <td>3.260</td>\n",
       "      <td>16</td>\n",
       "      <td>7</td>\n",
       "      <td>-0.258</td>\n",
       "      <td>9823.701</td>\n",
       "      <td>45239.16</td>\n",
       "      <td>10.972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>BHOD Volume Threshold: 90%</td>\n",
       "      <td>4836.910</td>\n",
       "      <td>4225.373</td>\n",
       "      <td>41.624</td>\n",
       "      <td>9939.385</td>\n",
       "      <td>-5102.475</td>\n",
       "      <td>1.948</td>\n",
       "      <td>471</td>\n",
       "      <td>723</td>\n",
       "      <td>0.651</td>\n",
       "      <td>...</td>\n",
       "      <td>46.8</td>\n",
       "      <td>-24.48</td>\n",
       "      <td>4910.570</td>\n",
       "      <td>3.260</td>\n",
       "      <td>14</td>\n",
       "      <td>7</td>\n",
       "      <td>-0.217</td>\n",
       "      <td>9794.557</td>\n",
       "      <td>45239.16</td>\n",
       "      <td>10.942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>BHOD Volume Threshold: 95%</td>\n",
       "      <td>4925.425</td>\n",
       "      <td>4349.640</td>\n",
       "      <td>42.028</td>\n",
       "      <td>9452.765</td>\n",
       "      <td>-4527.340</td>\n",
       "      <td>2.088</td>\n",
       "      <td>450</td>\n",
       "      <td>674</td>\n",
       "      <td>0.668</td>\n",
       "      <td>...</td>\n",
       "      <td>46.8</td>\n",
       "      <td>-24.48</td>\n",
       "      <td>4999.085</td>\n",
       "      <td>3.260</td>\n",
       "      <td>19</td>\n",
       "      <td>7</td>\n",
       "      <td>-0.183</td>\n",
       "      <td>9942.853</td>\n",
       "      <td>45239.16</td>\n",
       "      <td>10.613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>BHOD Volume Threshold: 100%</td>\n",
       "      <td>4118.470</td>\n",
       "      <td>3595.073</td>\n",
       "      <td>41.911</td>\n",
       "      <td>8357.655</td>\n",
       "      <td>-4239.185</td>\n",
       "      <td>1.972</td>\n",
       "      <td>407</td>\n",
       "      <td>617</td>\n",
       "      <td>0.660</td>\n",
       "      <td>...</td>\n",
       "      <td>46.8</td>\n",
       "      <td>-24.48</td>\n",
       "      <td>4214.970</td>\n",
       "      <td>3.260</td>\n",
       "      <td>19</td>\n",
       "      <td>7</td>\n",
       "      <td>-0.163</td>\n",
       "      <td>9906.130</td>\n",
       "      <td>45239.16</td>\n",
       "      <td>10.815</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>19 rows  24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Strategy ID  Net Profit ($)  Net Profit w Fees ($)  \\\n",
       "0    BHOD Volume Threshold: 10%        3166.620               1646.749   \n",
       "1    BHOD Volume Threshold: 15%        3607.510               2123.290   \n",
       "2    BHOD Volume Threshold: 20%        4073.300               2641.482   \n",
       "3    BHOD Volume Threshold: 25%        4426.755               3050.996   \n",
       "4    BHOD Volume Threshold: 30%        4976.085               3660.929   \n",
       "5    BHOD Volume Threshold: 35%        5166.500               3901.894   \n",
       "6    BHOD Volume Threshold: 40%        5549.320               4343.570   \n",
       "7    BHOD Volume Threshold: 45%        5918.890               4782.248   \n",
       "8    BHOD Volume Threshold: 50%        5909.005               4827.628   \n",
       "9    BHOD Volume Threshold: 55%        5702.845               4700.207   \n",
       "10   BHOD Volume Threshold: 60%        5539.180               4590.225   \n",
       "11   BHOD Volume Threshold: 65%        5688.460               4788.778   \n",
       "12   BHOD Volume Threshold: 70%        5767.495               4929.504   \n",
       "13   BHOD Volume Threshold: 75%        5540.440               4767.350   \n",
       "14   BHOD Volume Threshold: 80%        5203.485               4483.423   \n",
       "15   BHOD Volume Threshold: 85%        5191.860               4519.580   \n",
       "16   BHOD Volume Threshold: 90%        4836.910               4225.373   \n",
       "17   BHOD Volume Threshold: 95%        4925.425               4349.640   \n",
       "18  BHOD Volume Threshold: 100%        4118.470               3595.073   \n",
       "\n",
       "    Avg Share Size  Pos Trades Total ($)  Neg Trades Total ($)  Profit Factor  \\\n",
       "0           42.447             20201.290            -17034.670          1.186   \n",
       "1           42.330             19947.835            -16340.325          1.221   \n",
       "2           42.511             19538.535            -15465.235          1.263   \n",
       "3           42.302             19100.140            -14673.385          1.302   \n",
       "4           42.370             18472.940            -13496.855          1.369   \n",
       "5           42.301             17909.065            -12742.565          1.405   \n",
       "6           42.376             17271.580            -11722.260          1.473   \n",
       "7           42.481             16697.200            -10778.310          1.549   \n",
       "8           42.721             16089.085            -10180.080          1.580   \n",
       "9           42.409             15049.415             -9346.570          1.610   \n",
       "10          42.757             14263.845             -8724.665          1.635   \n",
       "11          42.891             13792.225             -8103.765          1.702   \n",
       "12          42.791             13062.090             -7294.595          1.791   \n",
       "13          42.890             12119.420             -6578.980          1.842   \n",
       "14          42.524             11390.750             -6187.265          1.841   \n",
       "15          41.974             10837.995             -5646.135          1.920   \n",
       "16          41.624              9939.385             -5102.475          1.948   \n",
       "17          42.028              9452.765             -4527.340          2.088   \n",
       "18          41.911              8357.655             -4239.185          1.972   \n",
       "\n",
       "    # Winning Trades  # Total Trades  Win Ratio  ...  Biggest Winner ($)  \\\n",
       "0               1092            1920      0.569  ...                48.9   \n",
       "1               1073            1868      0.574  ...                48.9   \n",
       "2               1040            1793      0.580  ...                48.9   \n",
       "3               1003            1718      0.584  ...                48.9   \n",
       "4                971            1629      0.596  ...                48.9   \n",
       "5                938            1560      0.601  ...                46.8   \n",
       "6                904            1477      0.612  ...                46.8   \n",
       "7                855            1383      0.618  ...                46.8   \n",
       "8                811            1310      0.619  ...                46.8   \n",
       "9                754            1212      0.622  ...                46.8   \n",
       "10               713            1141      0.625  ...                46.8   \n",
       "11               677            1075      0.630  ...                46.8   \n",
       "12               637             996      0.640  ...                46.8   \n",
       "13               591             915      0.646  ...                46.8   \n",
       "14               550             855      0.643  ...                46.8   \n",
       "15               518             796      0.651  ...                46.8   \n",
       "16               471             723      0.651  ...                46.8   \n",
       "17               450             674      0.668  ...                46.8   \n",
       "18               407             617      0.660  ...                46.8   \n",
       "\n",
       "    Biggest Loser ($)  Peak Equity ($)  Trough Equity ($)  Max Win Streak  \\\n",
       "0              -27.50         3689.935            -44.225              20   \n",
       "1              -27.50         3783.390            -23.825              19   \n",
       "2              -27.50         4257.785            -23.825              19   \n",
       "3              -27.50         4594.330             -3.800              21   \n",
       "4              -27.50         5081.060              3.260              19   \n",
       "5              -27.50         5275.245              3.260              18   \n",
       "6              -27.50         5701.850              3.260              17   \n",
       "7              -27.50         5997.960              3.260              17   \n",
       "8              -27.50         5994.645              3.260              18   \n",
       "9              -27.50         5801.385              3.260              19   \n",
       "10             -27.50         5616.520              3.260              19   \n",
       "11             -27.50         5775.275              3.260              19   \n",
       "12             -27.50         5858.990              3.260              23   \n",
       "13             -24.48         5590.035              3.260              20   \n",
       "14             -24.48         5283.680              3.260              19   \n",
       "15             -24.48         5287.560              3.260              16   \n",
       "16             -24.48         4910.570              3.260              14   \n",
       "17             -24.48         4999.085              3.260              19   \n",
       "18             -24.48         4214.970              3.260              19   \n",
       "\n",
       "    Max Lose Streak  Max Drawdown (%)  Avg Amount Used for Trade ($)  \\\n",
       "0                 8            -0.954                       9653.463   \n",
       "1                 8            -0.768                       9605.352   \n",
       "2                 7            -0.653                       9632.354   \n",
       "3                 7            -0.703                       9603.056   \n",
       "4                 6            -0.593                       9646.804   \n",
       "5                 9            -0.426                       9658.343   \n",
       "6                 8            -0.397                       9694.924   \n",
       "7                 7            -0.368                       9711.040   \n",
       "8                 7            -0.330                       9772.938   \n",
       "9                 7            -0.356                       9737.143   \n",
       "10                7            -0.300                       9809.781   \n",
       "11                7            -0.263                       9845.510   \n",
       "12                7            -0.226                       9830.087   \n",
       "13                7            -0.230                       9915.372   \n",
       "14                7            -0.295                       9876.176   \n",
       "15                7            -0.258                       9823.701   \n",
       "16                7            -0.217                       9794.557   \n",
       "17                7            -0.183                       9942.853   \n",
       "18                7            -0.163                       9906.130   \n",
       "\n",
       "    Max Amount Used for Trade ($)  Avg Holding Time (min)  \n",
       "0                        46670.85                   8.580  \n",
       "1                        46670.85                   8.602  \n",
       "2                        46670.85                   8.677  \n",
       "3                        46670.85                   8.884  \n",
       "4                        46670.85                   9.037  \n",
       "5                        46670.85                   9.113  \n",
       "6                        46670.85                   9.227  \n",
       "7                        45239.16                   9.386  \n",
       "8                        45239.16                   9.555  \n",
       "9                        45239.16                   9.806  \n",
       "10                       45239.16                   9.835  \n",
       "11                       45239.16                  10.082  \n",
       "12                       45239.16                  10.226  \n",
       "13                       45239.16                  10.462  \n",
       "14                       45239.16                  10.690  \n",
       "15                       45239.16                  10.972  \n",
       "16                       45239.16                  10.942  \n",
       "17                       45239.16                  10.613  \n",
       "18                       45239.16                  10.815  \n",
       "\n",
       "[19 rows x 24 columns]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics_table_test_TSLA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "badfc7ea-f799-433f-b86b-cfa7d2f9fba1",
   "metadata": {},
   "source": [
    "While the TSLA backtest took a bit longer, it's likely due to the additional number of trades taken for TSLA. Nevertheless, we see that for a baseline volume threshold of 10%, our metrics row is exactly the same as the previous metrics table we got for the event-driven backtest, confirming our process went smoothly. Using a charting software such as Trader Workstation or TradingView reveals similar results as well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4352289-a934-45d2-a8f6-2701d503f794",
   "metadata": {},
   "source": [
    "# Conclusions for Vectorized Backtesting\n",
    "While not shown here, this vectorized backtesting went through numerous iterations, with each one speeding up the backtest by a little bit. Those can be found in the archive folders in this repository. However, this current iteration is the fastest that we were able to make it, balancing efficiency with versatility. Not only can we alter most parameters in the broader BHOD strategy, but for certain features (like volume threshold), our program barely needs any extra time to run.\n",
    "\n",
    "Overall, we plan to use this backtest once we can obtain enough 1-minute data, and we want to see how it performs against certain Stocks in Play (SiP) that we choose every day. Moreover, using this backtesting method, we can truly analyze how well the Break High of Day Strategy does in general, and we can see how different data affects it. For example, it might work better for certain sectors of stocks than others, or it might work better for higher float stocks compared to lower float ones. Either way, we can run this backtest to see how well different stocks might perform, and we can get an unbiased idea of how to optimize this strategy afterward."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
